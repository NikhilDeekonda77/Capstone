import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID";
os.environ["CUDA_VISIBLE_DEVICES"]="4"; 
import pandas as pd
import torch
from PIL import Image
from torch.utils.data import Dataset
import torchvision.transforms as T
import torch.nn as nn
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torchvision.models as models
from tqdm import tqdm
import numpy as np
from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
import pickle
from pycocoevalcap.cider.cider import Cider
# class CXRDataset(Dataset):
#     def __init__(self, root, caption_file, transforms=None):
#         self.root = root
#         self.transforms = transforms
        
#         # Load captions based on UID
#         self.captions = self.load_captions(caption_file)
        
#         # Load all image files and map them to their corresponding UIDs
#         self.uid_image_map = self.create_uid_image_map(root, caption_file)
        
#         # Flatten the list to allow indexing and pair images with captions
#         self.imgs = [(uid, img) for uid, img_list in self.uid_image_map.items() for img in img_list]

#         self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
#         self.tokenizer.pad_token = self.tokenizer.eos_token

#     def load_captions(self, caption_file):
#         """ Load captions from CSV file (assuming uid in the first column, caption in another). """
#         df = pd.read_csv(caption_file)
#         # Create a dictionary with uid as key and caption as value
#         captions = pd.Series(df.caption.values, index=df.uid).to_dict()
#         return captions

#     def create_uid_image_map(self, root, caption_file):
#         """ 
#         Create a mapping from UID to images (multiple images can correspond to a single UID).
#         This assumes the CSV contains a column mapping UIDs to their corresponding image filenames.
#         """
#         df = pd.read_csv(caption_file)
#         uid_image_map = {}

#         for _, row in df.iterrows():
#             uid = row['uid']
#             image1 = row['Image 1']
#             image2 = row.get('Image 2', None)  # Handle optional second image

#             # Create a list of images for each UID
#             uid_image_map[uid] = []
#             if pd.notna(image1):
#                 uid_image_map[uid].append(image1)
#             if pd.notna(image2):
#                 uid_image_map[uid].append(image2)

#         return uid_image_map

#     def caption_to_tensor(self, caption):
#         """ Convert caption text to a tensor using GPT-2 tokenizer. """
#         encoded = self.tokenizer.encode(caption, add_special_tokens=True, 
#                                         max_length=300, padding='max_length', truncation=True)
#         return torch.tensor(encoded)

#     def __getitem__(self, idx):
#         # Get the UID and corresponding image filename from the flattened list
#         uid, img_filename = self.imgs[idx]
#         img_path = os.path.join(self.root, img_filename)
#         img = Image.open(img_path).convert("RGB")

#         # Get the caption for the current UID
#         caption = self.captions.get(uid, "")  # Fetch caption by UID

#         # Apply transforms if provided
#         if self.transforms is not None:
#             img = self.transforms(img)

#         # Convert the caption to a tensor
#         caption_tensor = self.caption_to_tensor(caption)

#         return img, caption_tensor

#     def __len__(self):
#         return len(self.imgs)  # Total number of images

class CXRDataset(Dataset):
    def __init__(self, root, caption_file, transforms=None):
        self.root = root
        self.transforms = transforms
        
        
        # # Load all image files and map them to their corresponding UIDs
        # self.uid_image_map = self.create_uid_image_map(root, caption_file)
        
        # # Flatten the list to allow indexing and pair images with captions
        # self.imgs = [(uid, img) for uid, img_list in self.uid_image_map.items() for img in img_list]

        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.data = pd.read_csv(caption_file)
        self.image_size = 224



    def caption_to_tensor(self, caption):
        """ Convert caption text to a tensor using GPT-2 tokenizer. """
        encoded = self.tokenizer.encode(caption, add_special_tokens=True, 
                                        max_length=300, padding='max_length', truncation=True)
        return torch.tensor(encoded)
        
    def __getitem__(self, idx):
        # Get the UID and corresponding image filename from the flattened list
        # uid, img_filename = self.imgs[idx]
        # print(uid)
        # print(img_filename)
        # img_path = os.path.join(self.root, img_filename)
        # img = Image.open(img_path).convert("RGB")

        caption = self.data.iloc[idx, 1]
        # print(img_name)
        # print(self.root)
        # print(self.data.iloc[idx, 2])
        # print(type(self.data.iloc[idx, 2]))
        # print(self.data.iloc[idx, 3])
        image_path1 = os.path.join(self.root,self.data.iloc[idx, 2])
        image_path2 = os.path.join(self.root, self.data.iloc[idx, 3])


        # Load three grayscale images
        image1 = Image.open(image_path1).resize((self.image_size, self.image_size)).convert('L')  # L mode is for grayscale
        image2 = Image.open(image_path2).resize((self.image_size, self.image_size)).convert('L')

        # Combine the three grayscale images into RGB
        img = Image.merge("RGB", (image1, image2, image1))


        # Get the caption for the current UID
        # caption = self.captions.get(idx, "")  # Fetch caption by UID

        # Apply transforms if provided
        if self.transforms is not None:
            img = self.transforms(img)

        # Convert the caption to a tensor
        caption_tensor = self.caption_to_tensor(caption)

        return img, caption_tensor

    def __len__(self):
        return len(self.data)  # Total number of images

# Example transform function
def get_transform(train):
    transforms = []
    transforms.append(T.Resize(size=(224, 224)))
    transforms.append(T.ToTensor())
    transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))
    return T.Compose(transforms)

# File paths for the captions
train_captions = './Train_captions.csv'
valid_captions = './Valid_captions.csv'
test_captions = './Test_captions.csv'

# train_captions = '/content/drive/MyDrive/Colab_Numerical_Methods/Train_captions.csv'
# valid_captions = '/content/drive/MyDrive/Colab_Numerical_Methods/Valid_captions.csv'
# test_captions = '/content/drive/MyDrive/Colab_Numerical_Methods/Test_captions.csv'


# Creating dataset objects with specific caption files
train_dataset = CXRDataset('./Images/Train/', train_captions, get_transform(train=True))
valid_dataset = CXRDataset('./Images/Valid/', valid_captions, get_transform(train=True))
test_dataset = CXRDataset('./Images/Test/', test_captions, get_transform(train=True))

# train_dataset = CXRDataset('/content/drive/MyDrive/Colab_Numerical_Methods/Train/', train_captions, get_transform(train=True))
# valid_dataset = CXRDataset('/content/drive/MyDrive/Colab_Numerical_Methods/Valid/', valid_captions, get_transform(train=True))
# test_dataset = CXRDataset('/content/drive/MyDrive/Colab_Numerical_Methods/Test/', test_captions, get_transform(train=True))

# Checking length of images and captions in the training set
print("Length of train set (images): ", len(train_dataset))
# print("Length of captions in train set: ", len(train_dataset.captions))
print("Length of valid set (images): ", len(valid_dataset))
# print("Length of captions in valid set: ", len(valid_dataset.captions))
print("Length of test set (images): ", len(test_dataset))
# print("Length of captions in test set: ", len(test_dataset.captions))

# Create DataLoader
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)

# # Checking batch sizes in DataLoader
# for imgs, captions in train_loader:
#     print("Batch of train images has shape: ", imgs.shape)
#     print("Batch of train captions has shape: ", captions.shape)
#     break



#decode generated captions to str
def output_to_words(outputs, tokenizer):
    """
    Convert the model outputs (logits) to words using the tokenizer.
    Args:
        outputs: Tensor of shape [batch_size, max_length, vocab_size], the model's output.
        tokenizer: The GPT-2 tokenizer to convert token IDs to words.
    Returns:
        List of generated sentences, one for each image in the batch.
    """
    # Step 1: Get the token IDs by selecting the index of the highest probability (argmax)
    predicted_token_ids = outputs.argmax(dim=2)  # Shape: [batch_size, max_length]
    # print(predicted_token_ids.shape)
    # Step 2: Convert token IDs to words
    generated_sentences = []
    for token_ids in predicted_token_ids:
        # Convert token IDs to a list of integers
        token_ids = token_ids.tolist()

        # Use the tokenizer to decode the token IDs to a string
        sentence = tokenizer.decode(token_ids, skip_special_tokens=True)

        # Add the generated sentence to the list
        generated_sentences.append(sentence)

    return generated_sentences



class AttentionBlock(nn.Module):
    def __init__(self, embedding_dim, num_heads=8):
        super(AttentionBlock, self).__init__()
        self.attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads)
        self.layer_norm = nn.LayerNorm(embedding_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim * 2),
            nn.ReLU(),
            nn.Linear(embedding_dim * 2, embedding_dim)
        )
       
    def forward(self, x):
        # Multi-head Attention
        attn_output, _ = self.attn(x, x, x)  # Self-attention
        x = self.layer_norm(x + attn_output)  # Add residual connection and normalize
       
        # Feed-forward network
        ffn_output = self.ffn(x)
        return self.layer_norm(x + ffn_output)  # Add residual connection again

class ReportGenerator(nn.Module):
    def __init__(self, resnet_model, gpt2_model, feature_dim=2048, max_length=300):
        super(ReportGenerator, self).__init__()
        self.resnet = resnet_model
        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling
        self.feature_proj = nn.Linear(feature_dim, gpt2_model.config.n_embd)
        self.attention_block = AttentionBlock(gpt2_model.config.n_embd)
        self.max_length = max_length
        self.gpt2 = gpt2_model
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

        self.positional_encoding = nn.Parameter(torch.randn(self.max_length, gpt2_model.config.n_embd))
        # Add a Transformer Encoder Layer
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=gpt2_model.config.n_embd, nhead=8),
            num_layers=2)

    def forward(self, image):
        # Step 1: Extract features from ResNet50
        # with torch.no_grad():
        resnet_features = self.resnet(image)  # [batch_size, 2048, H, W]
        resnet_features = self.pool(resnet_features)  # [batch_size, 2048, 1, 1]
        resnet_features = resnet_features.view(resnet_features.size(0), -1)  # [batch_size, 2048]

        # Step 2: Project ResNet50 features to GPT-2 embedding size
        resnet_features = self.feature_proj(resnet_features)  # [batch_size, embedding_dim]

        # Step 3: Refine features with AttentionBlock
        resnet_features = resnet_features.unsqueeze(1)  # Add sequence dimension [batch_size, 1, embedding_dim]
        refined_features = self.attention_block(resnet_features)  # [batch_size, 1, embedding_dim]

        # Step 4: Repeat features for max_length to simulate a sequence
        # refined_features = refined_features.repeat(1, self.max_length, 1)  # [batch_size, max_length, embedding_dim]

        refined_features = refined_features + self.positional_encoding.unsqueeze(0)  # [batch_size, max_length, embedding_dim]

        # Step 5: Pass through the transformer to simulate a sequence
        refined_features = self.transformer(refined_features.permute(1, 0, 2))  # [max_length, batch_size, embedding_dim]
        refined_features = refined_features.permute(1, 0, 2)

        # Step 5: Pass embeddings to GPT-2 using inputs_embeds
        gpt2_outputs = self.gpt2(
            inputs_embeds=refined_features
        )

        # Step 6: Return logits for cross-entropy loss
        return gpt2_outputs.logits  # [batch_size, max_length, vocab_size]


# Initialize ResNet50
resnet_model = models.resnet50(pretrained=True)
resnet_model = nn.Sequential(*list(resnet_model.children())[:-2])  # Remove the fully connected layer

# Initialize GPT-2
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2")

# # Define the model
# model = ReportGenerator(resnet_model, gpt2_model)

# # Example Inputs
# batch_size = 2
# image = torch.randn(batch_size, 3, 224, 224)  # Example batch of images

# # Forward Pass
# logits = model(image)  # Output shape: [batch_size, max_length, vocab_size]
# print("Logits shape:", logits.shape)




class ImageCaptionModel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, max_length):
        super(ImageCaptionModel, self).__init__()
        self.max_length = max_length

        # ResNet50 for feature extraction
        self.resnet = models.resnet50(pretrained=True)
        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)  # Adjust output size

        # LSTM for caption generation
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=1, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)  # Linear layer to output vocabulary size

        # Embedding layer to map token IDs to embeddings
        self.embedding = nn.Embedding(vocab_size, embed_size)

        # Load the tokenizer for processing
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    def forward(self, images):
        """
        Forward pass for training, generating captions based on images.
        Outputs a tensor of shape [batch_size, max_length, vocab_size].
        """
        batch_size = images.size(0)

        # Step 1: Extract features from the image
        features = self.resnet(images)  # [batch_size, embed_size]
        features = features.unsqueeze(1)  # Add time step: [batch_size, 1, embed_size]

        # Initialize the inputs for the LSTM (start with image features)
        inputs = features
        hidden_states = None  # LSTM's hidden state is initialized to None

        # Store the outputs at each time step (for cross-entropy loss calculation)
        outputs = []

        # Autoregressive caption generation loop
        for _ in range(self.max_length):
            lstm_out, hidden_states = self.lstm(inputs, hidden_states)  # [batch_size, 1, hidden_size]
            output = self.fc(lstm_out)  # [batch_size, 1, vocab_size]
            outputs.append(output)

            # Get the predicted token (using argmax for now, could be changed during training)
            predicted_token_ids = output.argmax(dim=2)  # [batch_size, 1]

            # Prepare the next input: use the predicted token as the next input to the LSTM
            inputs = self.embedding(predicted_token_ids)  # Convert predicted token to embedding

        # Stack all the outputs to form [batch_size, max_length, vocab_size]
        outputs = torch.cat(outputs, dim=1)  # [batch_size, max_length, vocab_size]

        return outputs  # This output can now be used for cross-entropy loss

# Example training function using cross-entropy loss
import nltk
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu

def train_and_validate(model, train_loader, valid_loader, criterion, optimizer, num_epochs):
    model.train()  # Set model to training mode
    epoch_train_loss = []  # To store training losses per epoch
    epoch_valid_loss = []  # To store validation losses per epoch
    best_bleu = 0

    for epoch in range(num_epochs):
        # Training phase
        train_total_loss = 0  # Initialize total loss for each epoch
        model.train()  # Ensure model is in training mode
        # for imgs, captions in tqdm(train_loader):
        #     imgs = imgs.to(device)
        #     captions = captions.to(device)  # Captions should be token IDs

        #     # Zero the parameter gradients
        #     optimizer.zero_grad()

        #     # Forward pass: Generate outputs of shape [batch_size, max_length, vocab_size]
        #     outputs = model(imgs)

        #     # Compute the cross-entropy loss
        #     loss = criterion(outputs.view(-1, outputs.size(2)), captions.view(-1))  # Flatten for loss computation
        #     train_total_loss += loss.item()  # Accumulate the loss

        #     loss.backward()
        #     optimizer.step()

        # # Calculate the average training loss
        # avg_train_loss = train_total_loss / len(train_loader)
        # print('Train Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, avg_train_loss))
        # epoch_train_loss.append(avg_train_loss)  # Store average training loss

        # Validation phase
        valid_total_loss = 0  # Initialize total validation loss
        model.eval()  # Set model to evaluation mode
        # all_predictions = []  # List to hold all predicted captions
        # all_references = []  # List to hold all reference captions
        if epoch>=0:
            with torch.no_grad():  # No need to track gradients during validation
                BLUE = []
                Rouge1 =[]
                Rouge2 =[]
                RougeL =[] 
                rouge = Rouge()
                for imgs, captions in tqdm(valid_loader):
                    imgs = imgs.to(device)
                    captions = captions.to(device)  # Captions should be token IDs

                    # Forward pass: Generate outputs of shape [batch_size, max_length, vocab_size]
                    outputs = model(imgs)

                    # Compute the cross-entropy loss
                    loss = criterion(outputs.view(-1, outputs.size(2)), captions.view(-1))  # Flatten for loss computation
                    valid_total_loss += loss.item()  # Accumulate the loss

                    # Get predicted captions (argmax)
                    _, predicted = torch.max(outputs, dim=2)

                    # Collect predictions and references for evaluation
                    # for i in range(predicted.size(0)):
                    #     pred_caption = predicted[i].cpu().numpy().tolist()
                    #     ref_caption = captions[i].cpu().numpy().tolist()

                        # all_predictions.append(pred_caption)  # Store predicted captions
                        # all_references.append(ref_caption)    # Store reference captions

                    generated_sentences = output_to_words(outputs, model.tokenizer) # You need to change the out to your models's output to get sentence
                    original_caption = model.tokenizer.batch_decode(captions, skip_special_tokens=True) # You need to change the out to your models's output to get sentence
                    
                    reference_tokens = [caption.split() for caption in original_caption]
                    generated_tokens =  [caption.split() for caption in generated_sentences]
                    print(reference_tokens)
                    print(len(reference_tokens))

                    print(generated_tokens)
                    print(len(generated_tokens))                    
                    print(oo)
                    # Compute BLEU score
                    bleu_score = corpus_bleu(reference_tokens, generated_tokens)
                    # bleu_score = sentence_bleu(reference_tokens, generated_tokens)
                    # print(f"BLEU score: {bleu_score:.4f}")
                    BLUE.append(bleu_score)
                    # Compute ROUGE scores
                    rouge_scores = rouge.get_scores(generated_sentences, original_caption, avg=True)
                # Compute BLEU and ROUGE scores
                    Rouge1.append(rouge_scores['rouge-1']['r'])
                    Rouge2.append(rouge_scores['rouge-2']['r'])
                    RougeL.append(rouge_scores['rouge-l']['r'])
                BLUE = np.array(BLUE)
                Rouge1 = np.array(Rouge1)
                Rouge2 = np.array(Rouge2)
                Rougel = np.array(RougeL)
                print(f"BLUE scores: {BLUE.mean()}")
                print(f"Rouge1 scores: {Rouge1.mean()}")
                print(f"Rouge2 scores: {Rouge2.mean()}")
                print(f"Rougel scores: {Rougel.mean()}")
                
            print('Testing complete.')
            print(generated_sentences[0])
            print(generated_sentences[1])             
                
                # # Print the generated sentences                
                # for i, sentence in enumerate(generated_sentences):
                #     print(f"Generated Caption for Image {i+1}: {sentence}")
                #     print('---'*50)
                #     print(f"True Caption for Image {i+1}: {original_caption}")
                #     # break

                    # print(pred_caption)
            # Calculate the average validation loss
            avg_valid_loss = valid_total_loss / len(valid_loader)
            print('Valid Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, avg_valid_loss))
            epoch_valid_loss.append(avg_valid_loss)  # Store average validation loss

            # # Optional: Calculate BLEU score (if desired)
            # bleu_scores = []
            # for ref, pred in zip(all_references, all_predictions):
            #     # Here you might need to decode token IDs to words
            #     score = sentence_bleu([ref], pred)
            #     bleu_scores.append(score)

            # avg_bleu_score = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0
            # print(f'Average BLEU score for validation in epoch {epoch + 1}: {avg_bleu_score:.4f}')
            avg_bleu_score = Rougel.mean()
            if avg_bleu_score > best_bleu:
                best_bleu = avg_bleu_score
                torch.save(model, './Weights/'+ str(epoch)+ '_'+ str(avg_bleu_score) + '_resnet50.pt')
                # torch.save(model, '/content/drive/MyDrive/Colab_Numerical_Methods/Weights/'+ str(epoch)+ '_'+ str(avg_bleu_score) + '.pt')
                print(f'New best BLEU score: {best_bleu:.4f}, saving model...')
            if epoch % 10 ==0:
                torch.save(model, './Weights/'+ str(epoch)+ '_'+ str(Rougel.mean()) + '_resnet50.pt')
            


    return model, epoch_train_loss, epoch_valid_loss

# # Main execution
# if __name__ == '__main__':
    # Parameters
embed_size = 256
hidden_size = 512
vocab_size = 50257  # GPT-2's vocabulary size
max_length = 300  # Maximum length of generated caption
num_epochs = 1000
batch_size = 16

# Initialize the model, criterion, and optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# model = ImageCaptionModel(vocab_size, embed_size, hidden_size, max_length).to(device)
# model = torch.load('./Weights/970_0.3041654861864532.pt')

# model = torch.load('./Weights/783_0.2481164066640733.pt')

new_gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2")
# Load the previously saved GPT-2 weights
# new_gpt2_model.load_state_dict(model.gpt2.state_dict(), strict=True)

resnet_model = models.resnet50(pretrained=True)
resnet_model = nn.Sequential(*list(resnet_model.children())[:-2])  # Remove the fully connected layer

# resnet_model.load_state_dict(model.resnet.state_dict(), strict=True)
 
model = ReportGenerator(resnet_model, new_gpt2_model).to(device)

model = torch.load('./Weights/970_0.25284502474330345.pt')
# Update the model with the loaded state_dict
# model.load_state_dict(state_dict)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)
# out = model(torch.randn(2, 3, 224, 224).to(device))

# Assuming `train_loader` and `valid_loader` are available
model, epoch_train_loss, epoch_valid_loss = train_and_validate(model, train_loader, valid_loader, criterion, optimizer, num_epochs)





# Example usage
BLUE1 = []
BLUE2 = []
BLUE3 = []
BLUE4 = []
Rouge1 =[]
Rouge2 =[]
RougeL =[]
METEOR = []
CIDERr = []
# Initialize SmoothingFunction
smoothie = SmoothingFunction()
# Initialize metric scorers
rouge = Rouge()
# cider_scorer = CiderScorer()
# Retrieved_re = Valid_df['Retrieved_caption'].tolist()
Valid_df = pd.read_csv('./Test_captions.csv')
Valid_captions = Valid_df['caption'].tolist()
cider_scorer = Cider()




with torch.no_grad():  # No need to track gradients during validation
    BLUE = []
    Rouge1 =[]
    Rouge2 =[]
    RougeL =[] 
    rouge = Rouge()
    idx = 0
    for imgs, captions in tqdm(test_loader):
        print(idx)
        imgs = imgs.to(device)
        captions = captions.to(device)  # Captions should be token IDs

        # Forward pass: Generate outputs of shape [batch_size, max_length, vocab_size]
        outputs = model(imgs)

        # Get predicted captions (argmax)
        _, predicted = torch.max(outputs, dim=2)

        generated_caption  = output_to_words(outputs, model.tokenizer) # You need to change the out to your models's output to get sentence
        # original_caption = model.tokenizer.batch_decode(captions, skip_special_tokens=True) # You need to change the out to your models's output to get sentence
 
        # print(generated_caption )
        # print(original_caption)
        # print('xxxx'*50)
        generated_caption = generated_caption[0]
        # generated_caption = generate_caption(Retrieved_re[num])
        Valid_df.loc[idx, 'Resnet50_LSTM_Generated_Caption'] =  generated_caption
        # Example generated and single reference caption
        # generated_caption = retrieved_caption
        reference_caption = Valid_captions[idx]

        # print(reference_caption)
        # print(io)
        # Tokenize the single reference caption for BLEU
        reference_tokens = [reference_caption.split()]
        generated_tokens = generated_caption.split()
        

        
        # Compute BLEU score
        # bleu_score = sentence_bleu(reference_tokens, generated_tokens)
        
        # Calculate BLEU-1
        bleu_1 = sentence_bleu(reference_tokens, generated_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie.method1)
        # print(f"BLEU-1: {bleu_1}")
        
        # Calculate BLEU-2
        bleu_2 = sentence_bleu(reference_tokens, generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie.method1)
        # print(f"BLEU-2: {bleu_2}")
        
        # Calculate BLEU-3
        bleu_3 = sentence_bleu(reference_tokens, generated_tokens, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smoothie.method1)
        # print(f"BLEU-3: {bleu_3}")
        
        # Calculate BLEU-4
        bleu_4 = sentence_bleu(reference_tokens, generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie.method1)
        # print(f"BLEU-4: {bleu_4}")

        # CID_score, scores = cider_scorer.compute_score({"image1": [reference_caption]}, {"image1": [generated_caption]})
        
        # print(f"CIDEr Score: {CID_score}")

        # Compute METEOR score
        mete_score = meteor_score(reference_tokens, generated_tokens)
        
        # print(f"METEOR Score: { mete_score:.4f}")

        # print(f"BLEU score: {bleu_score:.4f}")
        # BLUE.append(bleu_score)
        BLUE1.append(bleu_1)
        BLUE2.append(bleu_2)
        BLUE3.append(bleu_3)
        BLUE4.append(bleu_4)
        # Compute ROUGE scores
        rouge_scores = rouge.get_scores(generated_caption, reference_caption, avg=True)
        # print(f"ROUGE scores: {rouge_scores}")
        Rouge1.append(rouge_scores['rouge-1']['r'])
        Rouge2.append(rouge_scores['rouge-2']['r'])
        RougeL.append(rouge_scores['rouge-l']['r'])

        METEOR.append(mete_score)
        idx = idx + 1
        # CIDERr.append(CID_score)
        
        # print(iio)

# BLUE = np.array(BLUE)
BLUE1 = np.array(BLUE1)
BLUE2 = np.array(BLUE2)
BLUE3 = np.array(BLUE3)
BLUE4 = np.array(BLUE4)
Rouge1 = np.array(Rouge1)
Rouge2 = np.array(Rouge2)
Rougel = np.array(RougeL)
METEOR = np.array(METEOR)
# CIDERr = np.array(CIDERr)
print(f"BLUE1 scores: {BLUE1.mean()}")
print(f"BLUE2 scores: {BLUE2.mean()}")
print(f"BLUE3 scores: {BLUE3.mean()}")
print(f"BLUE4 scores: {BLUE4.mean()}")
print(f"Rouge1 scores: {Rouge1.mean()}")
print(f"Rouge2 scores: {Rouge2.mean()}")
print(f"Rougel scores: {Rougel.mean()}")
print(f"METEOR scores: {METEOR.mean()}")
# print(f"CIDERr scores: {CIDERr.mean()}")


num_images = len(Valid_df['caption'].tolist())
image_ids = [f"image{i+1}" for i in range(num_images)]

# Prepare references and candidates
references = {}
candidates = {}

for image_id, true, generated_caption in zip(image_ids, Valid_df['caption'].tolist(), Valid_df['Resnet50_LSTM_Generated_Caption'].tolist()):
    # Using the same caption as both reference and candidate for demonstration
    references[image_id] = [true]  # List of references
    candidates[image_id] = [generated_caption]   # List of candidates

# # Now references and candidates are in the required format
# print("References:", references)
# print("Candidates:", candidates)

# Compute the CIDEr score
score, scores = cider_scorer.compute_score(references, candidates)

print(f"CIDEr Score: {score:.4f}")
# print(f"Detailed Scores: {scores}")
Valid_df.to_csv('Resnet50_Caption_test.csv', index=False)
