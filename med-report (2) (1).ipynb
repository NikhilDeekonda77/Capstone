{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7abb6e8905d24f0f83cdf18152a7affa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffa0b660ac394e92aeceb07014f44527","IPY_MODEL_5c483b77b3bf44a98f1b44f9268c4706","IPY_MODEL_49deb2a8f0004843a66fef0dd6cf2b96"],"layout":"IPY_MODEL_66d98a9c22bd4f70af9d23f56bd61df2"}},"ffa0b660ac394e92aeceb07014f44527":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_148699cd78a94db8bb329d8986004b7d","placeholder":"​","style":"IPY_MODEL_46e048de3ae84d7496f2159c6930a852","value":"Loading checkpoint shards: 100%"}},"5c483b77b3bf44a98f1b44f9268c4706":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e42e8b3a1aaa45779bcdcaa65a9119af","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36abf7d2e7b94e7dbf5831ca0b5cab4c","value":2}},"49deb2a8f0004843a66fef0dd6cf2b96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aac5edb567cb4fd993b0cb1ae9aec3d1","placeholder":"​","style":"IPY_MODEL_a3e841bfdf7740349dc1fc7ef4db9334","value":" 2/2 [00:08&lt;00:00,  3.97s/it]"}},"66d98a9c22bd4f70af9d23f56bd61df2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"148699cd78a94db8bb329d8986004b7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46e048de3ae84d7496f2159c6930a852":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e42e8b3a1aaa45779bcdcaa65a9119af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36abf7d2e7b94e7dbf5831ca0b5cab4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aac5edb567cb4fd993b0cb1ae9aec3d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3e841bfdf7740349dc1fc7ef4db9334":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10716710,"sourceType":"datasetVersion","datasetId":6642655}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e88083e1-10b8-4778-ba2c-db52ec8b7d88","cell_type":"code","source":["pip install -U bitsandbytes rouge_score nltk pycocoevalcap transformers"],"metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"e88083e1-10b8-4778-ba2c-db52ec8b7d88","executionInfo":{"status":"ok","timestamp":1742228395067,"user_tz":240,"elapsed":91852,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"67463771-7c7c-4c14-88ae-f4bc78f746ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Collecting pycocoevalcap\n","  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Collecting transformers\n","  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=ece8336dbfc411a284ac8be4f2ee1462bbde91ff07dbf4feb5359d1f219926ce\n","  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n","Successfully built rouge_score\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, rouge_score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, pycocoevalcap, bitsandbytes\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.48.3\n","    Uninstalling transformers-4.48.3:\n","      Successfully uninstalled transformers-4.48.3\n","Successfully installed bitsandbytes-0.45.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pycocoevalcap-1.2 rouge_score-0.1.2 transformers-4.49.0\n"]}],"execution_count":null},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"deJNGh0MFEC6","executionInfo":{"status":"ok","timestamp":1742228436835,"user_tz":240,"elapsed":41765,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"ab0cce37-c380-49e6-dd67-a6223e4d93aa"},"id":"deJNGh0MFEC6","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"id":"685c924a-73ef-4949-9dea-22ba3e518b51","cell_type":"code","source":["import os\n","import torch\n","import pandas as pd\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import (\n","    AutoProcessor,\n","    AutoModel,\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig\n",")\n","from peft import LoraConfig, get_peft_model\n","from PIL import Image\n","\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","# Constants\n","BATCH_SIZE = 1\n","GRAD_ACCUM_STEPS = 8\n","MAX_LEN = 300\n","\n","vision_encoder = AutoModel.from_pretrained(\n","    \"facebook/dinov2-base\"\n",").half().to(\"cuda\")\n","\n","for name, param in vision_encoder.named_parameters():\n","    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n","        param.requires_grad = True\n","    else:\n","        param.requires_grad = False\n","\n","mistral_tokenizer = AutoTokenizer.from_pretrained(\n","    \"mistralai/Mistral-7B-v0.1\",\n","    padding_side=\"right\",\n","    use_fast=True\n",")\n","mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n","\n","text_decoder = AutoModelForCausalLM.from_pretrained(\n","    \"mistralai/Mistral-7B-v0.1\",\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    trust_remote_code=True\n",")\n","text_decoder.config.pad_token_id = mistral_tokenizer.pad_token_id\n","for name, param in text_decoder.named_parameters():\n","    if (\"model.layers.30\" in name or \"model.layers.31\" in name) and \\\n","       param.dtype in [torch.float32, torch.float16, torch.bfloat16]:\n","        param.requires_grad = True\n","    else:\n","        param.requires_grad = False\n","\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\n","        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"\n","    ],\n","    layers_to_transform=[30, 31],\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","text_decoder = get_peft_model(text_decoder, lora_config)\n","\n","with torch.no_grad():\n","    for name, param in text_decoder.named_parameters():\n","        if \"lora_\" in name and param.dtype == torch.float32:\n","            param.data = param.data.half()\n","\n","class ProjectionLayer(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.proj = nn.Linear(768, 4096)\n","        self.gelu = nn.GELU()\n","\n","    def forward(self, x):\n","        return self.gelu(self.proj(x))\n","\n","projection = ProjectionLayer().half().to(\"cuda\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T21:44:04.957819Z","iopub.execute_input":"2025-02-12T21:44:04.958193Z","iopub.status.idle":"2025-02-12T21:48:07.550692Z","shell.execute_reply.started":"2025-02-12T21:44:04.958165Z","shell.execute_reply":"2025-02-12T21:48:07.549760Z"},"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["7abb6e8905d24f0f83cdf18152a7affa","ffa0b660ac394e92aeceb07014f44527","5c483b77b3bf44a98f1b44f9268c4706","49deb2a8f0004843a66fef0dd6cf2b96","66d98a9c22bd4f70af9d23f56bd61df2","148699cd78a94db8bb329d8986004b7d","46e048de3ae84d7496f2159c6930a852","e42e8b3a1aaa45779bcdcaa65a9119af","36abf7d2e7b94e7dbf5831ca0b5cab4c","aac5edb567cb4fd993b0cb1ae9aec3d1","a3e841bfdf7740349dc1fc7ef4db9334"]},"id":"685c924a-73ef-4949-9dea-22ba3e518b51","executionInfo":{"status":"ok","timestamp":1742231534320,"user_tz":240,"elapsed":9432,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"340cc85c-a1ec-4bd9-a4ba-2309a3df1915"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7abb6e8905d24f0f83cdf18152a7affa"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rTraining Epoch 1/10:   0%|          | 0/2678 [38:32<?, ?it/s, Train Loss=0.1379]\n"]}],"execution_count":null},{"id":"5c574cd6-ba0a-43e6-b37e-d062d79755f0","cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset\n","from PIL import Image\n","from transformers import AutoTokenizer\n","\n","MAX_LEN = 300\n","\n","class CXRMultiViewDataset(Dataset):\n","    def __init__(self, root, caption_file, processor, tokenizer_name=\"medalpaca/medalpaca-7b\"):\n","        self.root = root\n","        self.data = pd.read_csv(caption_file)\n","        self.processor = processor\n","        self.image_size = 224\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","        self.tokenizer.padding_side = \"right\"\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","        caption = row[\"caption\"]\n","\n","        # Use your exact column names: \"Image 1\" and \"Image 2\"\n","        image_path_front = os.path.join(self.root, row[\"Image 1\"])\n","        image_path_lat   = os.path.join(self.root, row[\"Image 2\"])\n","\n","        # Load, resize, and convert grayscale to 3-channel as needed\n","        front_img = Image.open(image_path_front).convert('L').resize((self.image_size, self.image_size))\n","        lat_img   = Image.open(image_path_lat).convert('L').resize((self.image_size, self.image_size))\n","\n","        front_arr = np.array(front_img, dtype=np.float32) / 255.0\n","        front_arr = np.stack([front_arr, front_arr, front_arr], axis=-1)\n","        front_tensor = torch.tensor(front_arr).permute(2, 0, 1)\n","\n","        lat_arr = np.array(lat_img, dtype=np.float32) / 255.0\n","        lat_arr = np.stack([lat_arr, lat_arr, lat_arr], axis=-1)\n","        lat_tensor = torch.tensor(lat_arr).permute(2, 0, 1)\n","\n","        front_encoding = self.processor(images=front_tensor, return_tensors=\"pt\")\n","        lat_encoding   = self.processor(images=lat_tensor,   return_tensors=\"pt\")\n","\n","        # Tokenize caption\n","        caption_tokens = self.tokenizer(\n","            caption,\n","            return_tensors=\"pt\",\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=300,\n","            add_special_tokens=True\n","        )\n","\n","        return {\n","            \"pixel_values_front\": front_encoding[\"pixel_values\"].squeeze(0),\n","            \"pixel_values_lat\":   lat_encoding[\"pixel_values\"].squeeze(0),\n","            \"input_ids\":          caption_tokens[\"input_ids\"].squeeze(0),\n","            \"attention_mask\":     caption_tokens[\"attention_mask\"].squeeze(0),\n","            # Optionally store raw reference for metric evaluations\n","            \"references\":         caption\n","        }\n","\n","\n","def multi_view_collate_fn(batch):\n","    return {\n","        \"pixel_values_front\": torch.stack([x[\"pixel_values_front\"] for x in batch]),\n","        \"pixel_values_lat\":   torch.stack([x[\"pixel_values_lat\"]   for x in batch]),\n","        \"input_ids\":          torch.stack([x[\"input_ids\"]          for x in batch]),\n","        \"attention_mask\":     torch.stack([x[\"attention_mask\"]     for x in batch]),\n","        \"references\":         [x[\"references\"] for x in batch]\n","\n","    }\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T21:48:40.517917Z","iopub.execute_input":"2025-02-12T21:48:40.518280Z","iopub.status.idle":"2025-02-12T21:48:40.527351Z","shell.execute_reply.started":"2025-02-12T21:48:40.518247Z","shell.execute_reply":"2025-02-12T21:48:40.526476Z"},"id":"5c574cd6-ba0a-43e6-b37e-d062d79755f0"},"outputs":[],"execution_count":null},{"id":"80aa1c31-541d-4a06-bbe4-7f4913434f13","cell_type":"code","source":["train_dataset = CXRMultiViewDataset('/content/drive/MyDrive/Small_human_extracted/Images/Train/', '/content/drive/MyDrive/Small_human_extracted/Train_captions.csv', processor=AutoProcessor.from_pretrained(\"facebook/dinov2-base\",\n","        do_rescale=False ))\n","valid_dataset = CXRMultiViewDataset('/content/drive/MyDrive/Small_human_extracted/Images/Valid/', '/content/drive/MyDrive/Small_human_extracted/Valid_captions.csv', processor=AutoProcessor.from_pretrained(\"facebook/dinov2-base\",\n","        do_rescale=False ))\n","test_dataset = CXRMultiViewDataset('/content/drive/MyDrive/Small_human_extracted/Images/Test/', '/content/drive/MyDrive/Small_human_extracted/Test_captions.csv', processor=AutoProcessor.from_pretrained(\"facebook/dinov2-base\",\n","        do_rescale=False ))\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=8,\n","    shuffle=True,\n","    collate_fn=multi_view_collate_fn,\n","    pin_memory=True\n",")\n","\n","valid_loader = DataLoader(\n","    valid_dataset,\n","    batch_size=8,\n","    shuffle=True,\n","    collate_fn=multi_view_collate_fn,\n","    pin_memory=True\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=8,\n","    shuffle=False,\n","    collate_fn=multi_view_collate_fn,\n","    pin_memory=True\n",")\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:03:01.296590Z","iopub.execute_input":"2025-02-12T22:03:01.296882Z","iopub.status.idle":"2025-02-12T22:03:04.974633Z","shell.execute_reply.started":"2025-02-12T22:03:01.296860Z","shell.execute_reply":"2025-02-12T22:03:04.973960Z"},"id":"80aa1c31-541d-4a06-bbe4-7f4913434f13"},"outputs":[],"execution_count":null},{"id":"b409245c-a5d7-4d68-9e10-e34b92aff333","cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class CXRReportGenerator(nn.Module):\n","    def __init__(self, vision_encoder, text_decoder, projection):\n","        super().__init__()\n","        self.vision_encoder = vision_encoder\n","        self.text_decoder = text_decoder\n","        self.projection = projection\n","\n","    def forward(\n","        self,\n","        pixel_values_front=None,\n","        pixel_values_lat=None,\n","        input_ids=None,\n","        attention_mask=None,\n","        labels=None,\n","        max_new_tokens=256\n","    ):\n","        device = pixel_values_front.device\n","\n","        if input_ids is not None:\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","        if labels is not None:\n","            labels = labels.to(device)\n","\n","        vision_outputs_front = self.vision_encoder(pixel_values_front).last_hidden_state  # (B, seq_len, 768)\n","        front_cls = vision_outputs_front[:, 0, :]  # (B, 768)\n","\n","        vision_outputs_lat = self.vision_encoder(pixel_values_lat).last_hidden_state      # (B, seq_len, 768)\n","        lat_cls = vision_outputs_lat[:, 0, :]  # (B, 768)\n","\n","        combined_cls = 0.5 * (front_cls + lat_cls)  # shape: (B, 768)\n","\n","        projected_vision = self.projection(combined_cls)  # e.g. (B, 4096)\n","\n","        if input_ids is not None:\n","            text_embeds = self.text_decoder.model.get_input_embeddings()(input_ids)  # (B, seq_len, 4096)\n","\n","\n","            vision_prefix = projected_vision.unsqueeze(1)\n","\n","            inputs_embeds = torch.cat([vision_prefix, text_embeds], dim=1)\n","\n","            batch_size = pixel_values_front.size(0)\n","            prefix_mask = torch.ones(batch_size, 1, device=device)\n","            combined_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n","\n","            if labels is not None:\n","                shifted_labels = torch.cat([\n","                    torch.full((batch_size, 1), -100, device=device),\n","                    labels\n","                ], dim=1)\n","            else:\n","                shifted_labels = None\n","\n","            outputs = self.text_decoder(\n","                inputs_embeds=inputs_embeds,\n","                attention_mask=combined_attention_mask,\n","                labels=shifted_labels\n","            )\n","            return outputs.loss\n","\n","        else:\n","\n","            vision_prefix = projected_vision.unsqueeze(1)  # (B, 1, 4096)\n","            generated = self.text_decoder.generate(\n","                inputs_embeds=vision_prefix,\n","                max_new_tokens=max_new_tokens,\n","                temperature=0.7,\n","                top_k=50,\n","                do_sample=True,\n","                pad_token_id=self.text_decoder.config.pad_token_id\n","            )\n","            return generated\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:14:31.475023Z","iopub.execute_input":"2025-02-12T22:14:31.475354Z","iopub.status.idle":"2025-02-12T22:14:31.483057Z","shell.execute_reply.started":"2025-02-12T22:14:31.475327Z","shell.execute_reply":"2025-02-12T22:14:31.482188Z"},"id":"b409245c-a5d7-4d68-9e10-e34b92aff333"},"outputs":[],"execution_count":null},{"id":"b34d708b-820a-42bc-8b59-88a50987bdbe","cell_type":"code","source":["import bitsandbytes as bnb\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = CXRReportGenerator(\n","    vision_encoder=vision_encoder,\n","    text_decoder=text_decoder,\n","    projection=projection\n",")\n","model.to(device)\n","\n","trainable_proj    = list(model.projection.parameters())\n","trainable_decoder = [p for p in model.text_decoder.parameters() if p.requires_grad]\n","trainable_encoder = [p for p in model.vision_encoder.parameters() if p.requires_grad]\n","\n","optimizer = bnb.optim.Adam8bit(\n","    [\n","        {'params': trainable_proj,    'lr': 1e-4},\n","        {'params': trainable_decoder, 'lr': 1e-4, 'weight_decay': 0.01},\n","        {'params': trainable_encoder, 'lr': 1e-5}\n","    ],\n","    betas=(0.9, 0.999),\n","    optim_bits=8\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:17:12.535505Z","iopub.execute_input":"2025-02-12T22:17:12.535826Z","iopub.status.idle":"2025-02-12T22:17:12.569394Z","shell.execute_reply.started":"2025-02-12T22:17:12.535801Z","shell.execute_reply":"2025-02-12T22:17:12.568754Z"},"id":"b34d708b-820a-42bc-8b59-88a50987bdbe"},"outputs":[],"execution_count":null},{"id":"103e3ce0-1af8-4bfb-9965-fdf19a0d501d","cell_type":"code","source":["from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","from pycocoevalcap.cider.cider import Cider\n","import nltk\n","import numpy as np\n","import gc\n","from tqdm import tqdm\n","nltk.download('punkt')\n","\n","def calculate_metrics(predictions, references):\n","    # Tokenize for BLEU and CIDEr\n","    refs_bleu = [[nltk.word_tokenize(ref)] for ref in references]\n","    hyps_bleu = [nltk.word_tokenize(pred) for pred in predictions]\n","\n","    # BLEU-4\n","    bleu4 = corpus_bleu(\n","        refs_bleu, hyps_bleu,\n","        weights=(0.25, 0.25, 0.25, 0.25),\n","        smoothing_function=SmoothingFunction().method4\n","    )\n","\n","    # ROUGE-L\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    rouge_l = np.mean([rouge.score(ref, hyp)['rougeL'].fmeasure\n","                     for ref, hyp in zip(references, predictions)])\n","\n","    # CIDEr\n","    cider = Cider()\n","    refs_cider = {i: [ref] for i, ref in enumerate(references)}\n","    hyps_cider = {i: [hyp] for i, hyp in enumerate(predictions)}\n","    cider_score, _ = cider.compute_score(refs_cider, hyps_cider)\n","\n","    return bleu4, rouge_l, cider_score\n","\n","def train_and_validate(\n","    model,\n","    train_loader,\n","    val_loader,\n","    optimizer,\n","    tokenizer,\n","    num_epochs=5,\n","    device='cuda',\n","    save_path=\"best_model.pt\"\n","):\n","    best_bleu4 = 0.0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_train_loss = 0.0\n","\n","        train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")\n","\n","        for step, batch in enumerate(train_loader):\n","            # Clear CUDA cache if needed (optional, can help with memory fragmentation)\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","            # Move to GPU\n","            pixel_values_front = batch[\"pixel_values_front\"].to(device)\n","            pixel_values_lat   = batch[\"pixel_values_lat\"].to(device)\n","            input_ids          = batch[\"input_ids\"].to(device)\n","            attention_mask     = batch[\"attention_mask\"].to(device)\n","            labels             = batch[\"input_ids\"].to(device)\n","\n","            # Forward pass\n","            loss = model(\n","                pixel_values_front=pixel_values_front,\n","                pixel_values_lat=pixel_values_lat,\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            # Backprop\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_train_loss += loss.item()\n","\n","            if (step + 1) % 50 == 0:\n","                print(f\"[Epoch {epoch+1}/{num_epochs} - Step {step+1}/{len(train_loader)}] \"\n","                      f\"Train Loss: {loss.item():.4f}\")\n","\n","            train_pbar.set_postfix({\n","                \"Train Loss\": f\"{loss.item():.4f}\"\n","            })\n","\n","        avg_train_loss = total_train_loss / len(train_loader)\n","        print(f\"Epoch {epoch+1}/{num_epochs} - Average Train Loss: {avg_train_loss:.4f}\")\n","\n","        model.eval()\n","        val_predictions = []\n","        val_references = []\n","        total_val_loss = 0.0\n","\n","        val_pbar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\")\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                pixel_values_front = batch[\"pixel_values_front\"].to(device)\n","                pixel_values_lat   = batch[\"pixel_values_lat\"].to(device)\n","                input_ids      = batch[\"input_ids\"].to(device)\n","                attention_mask = batch[\"attention_mask\"].to(device)\n","                labels = batch[\"input_ids\"].to(device)\n","\n","                val_loss = model(\n","                    pixel_values_front=pixel_values_front,\n","                    pixel_values_lat=pixel_values_lat,\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    labels=labels\n","                )\n","                total_val_loss += val_loss.item()\n","\n","                generated_ids = model(\n","                    pixel_values_front=pixel_values_front,\n","                    pixel_values_lat=pixel_values_lat\n","                )\n","\n","                preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","\n","                val_predictions.extend(preds)\n","                references = batch[\"references\"]  # list of strings\n","                val_references.extend(references)\n","\n","                val_pbar.set_postfix({\n","                    \"Val Loss\": f\"{val_loss.item():.4f}\"\n","                })\n","\n","            # Compute average validation loss\n","            avg_val_loss = total_val_loss / len(val_loader)\n","\n","        # Calculate metrics on validation set\n","        bleu4, rouge_l, cider_score = calculate_metrics(val_predictions, val_references)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n","              f\"Train Loss: {avg_train_loss:.4f} | \"\n","              f\"Val Loss: {avg_val_loss:.4f} | \"\n","              f\"BLEU-4: {bleu4:.4f} | \"\n","              f\"ROUGE-L: {rouge_l:.4f} | \"\n","              f\"CIDEr: {cider_score:.4f}\")\n","\n","        # Check if current BLEU-4 is the best so far\n","        if bleu4 > best_bleu4:\n","            best_bleu4 = bleu4\n","            print(f\"New best BLEU-4 ({best_bleu4:.4f}) - saving model...\")\n","            torch.save(model.state_dict(), save_path)\n","\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:17:25.729784Z","iopub.execute_input":"2025-02-12T22:17:25.730095Z","iopub.status.idle":"2025-02-12T22:17:25.744263Z","shell.execute_reply.started":"2025-02-12T22:17:25.730072Z","shell.execute_reply":"2025-02-12T22:17:25.743422Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"103e3ce0-1af8-4bfb-9965-fdf19a0d501d","executionInfo":{"status":"ok","timestamp":1742233273934,"user_tz":240,"elapsed":10,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"f740bcee-c1d5-4a60-fb65-65f5cfbf27bd"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"execution_count":null},{"id":"84109d32-f8cf-4966-99f0-634487d0991a","cell_type":"code","source":["train_and_validate(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=valid_loader,\n","    optimizer=optimizer,\n","    tokenizer=mistral_tokenizer,\n","    num_epochs=10,\n","    device=device,\n","    save_path=\"/content/drive/MyDrive/Small_human_extracted/best_rg_model.pt\"\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T22:17:26.415679Z","iopub.execute_input":"2025-02-12T22:17:26.415961Z","iopub.status.idle":"2025-02-12T22:17:26.638106Z","shell.execute_reply.started":"2025-02-12T22:17:26.415940Z","shell.execute_reply":"2025-02-12T22:17:26.636003Z"},"colab":{"base_uri":"https://localhost:8080/","height":689},"id":"84109d32-f8cf-4966-99f0-634487d0991a","executionInfo":{"status":"error","timestamp":1742233280615,"user_tz":240,"elapsed":6124,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"f08aaf81-746c-48a8-ea7d-47ef5e97d6f8"},"outputs":[{"output_type":"stream","name":"stderr","text":["\n","Training Epoch 1/10:   0%|          | 0/335 [00:00<?, ?it/s]\u001b[A<ipython-input-50-f198ba72e583>:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with amp.autocast():\n","\n","Training Epoch 1/10:   0%|          | 0/335 [00:00<?, ?it/s, Loss=4.1163, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   0%|          | 1/335 [00:00<04:17,  1.30it/s, Loss=4.1163, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   0%|          | 1/335 [00:01<04:17,  1.30it/s, Loss=5.0413, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   1%|          | 2/335 [00:01<04:15,  1.30it/s, Loss=5.0413, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   1%|          | 2/335 [00:02<04:15,  1.30it/s, Loss=4.4541, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   1%|          | 3/335 [00:02<04:14,  1.30it/s, Loss=4.4541, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   1%|          | 3/335 [00:03<04:14,  1.30it/s, Loss=5.1722, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   1%|          | 4/335 [00:03<04:18,  1.28it/s, Loss=5.1722, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   1%|          | 4/335 [00:03<04:18,  1.28it/s, Loss=4.8952, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   1%|▏         | 5/335 [00:03<04:12,  1.31it/s, Loss=4.8952, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   1%|▏         | 5/335 [00:04<04:12,  1.31it/s, Loss=4.3047, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   2%|▏         | 6/335 [00:04<04:09,  1.32it/s, Loss=4.3047, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   2%|▏         | 6/335 [00:05<04:09,  1.32it/s, Loss=4.8779, LR=0.00e+00]\u001b[A\n","Training Epoch 1/10:   2%|▏         | 7/335 [00:06<04:45,  1.15it/s, Loss=4.8779, LR=0.00e+00]\n"]},{"output_type":"error","ename":"ValueError","evalue":"Attempting to unscale FP16 gradients.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-c50a65be7140>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_and_validate(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-f198ba72e583>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, optimizer, tokenizer, num_epochs, device, save_path, grad_accum_steps)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# Gradient accumulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgrad_accum_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    258\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mallow_fp16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting to unscale FP16 gradients.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                         \u001b[0;31m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."]}],"execution_count":null},{"cell_type":"code","source":["ckpt = torch.load(\"/content/drive/MyDrive/Small_human_extracted/best_rg_model.pt\", map_location=device)\n","\n","model.load_state_dict(ckpt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AW47ck5cYLU","executionInfo":{"status":"ok","timestamp":1739463526201,"user_tz":300,"elapsed":5437,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"656a34af-93fa-47d4-89f9-d7185e726e19"},"id":"3AW47ck5cYLU","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-87c8e69f9913>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ckpt = torch.load(\"/content/drive/MyDrive/Small_human_extracted/best_cxr_model.pt\", map_location=device)\n"]},{"output_type":"execute_result","data":{"text/plain":["_IncompatibleKeys(missing_keys=[], unexpected_keys=['text_decoder.base_model.model.model.layers.0.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.0.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.0.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.0.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.0.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.0.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.0.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.0.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.0.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.0.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.0.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.0.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.0.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.0.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.0.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.0.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.0.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.0.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.0.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.0.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.0.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.0.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.0.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.0.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.0.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.0.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.0.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.0.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.0.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.0.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.0.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.0.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.1.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.1.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.1.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.1.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.1.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.1.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.1.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.1.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.1.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.1.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.1.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.1.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.1.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.1.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.1.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.1.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.1.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.1.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.1.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.1.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.1.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.1.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.1.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.1.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.1.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.1.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.1.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.1.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.1.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.1.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.1.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.1.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.1.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.1.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.1.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.2.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.2.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.2.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.2.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.2.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.2.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.2.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.2.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.2.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.2.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.2.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.2.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.2.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.2.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.2.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.2.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.2.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.2.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.2.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.2.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.2.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.2.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.2.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.2.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.2.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.2.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.2.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.2.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.2.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.2.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.2.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.2.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.2.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.2.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.2.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.3.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.3.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.3.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.3.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.3.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.3.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.3.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.3.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.3.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.3.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.3.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.3.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.3.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.3.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.3.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.3.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.3.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.3.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.3.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.3.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.3.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.3.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.3.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.3.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.3.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.3.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.3.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.3.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.3.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.3.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.3.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.3.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.3.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.3.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.3.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.4.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.4.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.4.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.4.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.4.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.4.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.4.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.4.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.4.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.4.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.4.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.4.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.4.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.4.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.4.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.4.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.4.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.4.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.4.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.4.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.4.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.4.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.4.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.4.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.4.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.4.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.4.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.4.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.4.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.4.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.4.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.4.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.4.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.4.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.4.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.5.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.5.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.5.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.5.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.5.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.5.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.5.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.5.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.5.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.5.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.5.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.5.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.5.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.5.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.5.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.5.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.5.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.5.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.5.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.5.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.5.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.5.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.5.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.5.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.5.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.5.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.5.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.5.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.5.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.5.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.5.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.5.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.5.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.5.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.5.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.6.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.6.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.6.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.6.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.6.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.6.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.6.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.6.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.6.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.6.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.6.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.6.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.6.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.6.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.6.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.6.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.6.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.6.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.6.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.6.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.6.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.6.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.6.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.6.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.6.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.6.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.6.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.6.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.6.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.6.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.6.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.6.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.6.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.6.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.6.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.7.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.7.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.7.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.7.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.7.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.7.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.7.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.7.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.7.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.7.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.7.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.7.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.7.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.7.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.7.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.7.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.7.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.7.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.7.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.7.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.7.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.7.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.7.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.7.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.7.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.7.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.7.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.7.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.7.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.7.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.7.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.7.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.7.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.7.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.7.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.8.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.8.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.8.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.8.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.8.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.8.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.8.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.8.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.8.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.8.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.8.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.8.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.8.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.8.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.8.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.8.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.8.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.8.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.8.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.8.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.8.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.8.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.8.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.8.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.8.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.8.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.8.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.8.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.8.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.8.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.8.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.8.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.8.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.8.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.8.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.9.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.9.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.9.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.9.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.9.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.9.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.9.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.9.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.9.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.9.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.9.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.9.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.9.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.9.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.9.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.9.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.9.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.9.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.9.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.9.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.9.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.9.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.9.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.9.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.9.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.9.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.9.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.9.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.9.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.9.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.9.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.9.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.9.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.9.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.9.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.10.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.10.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.10.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.10.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.10.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.10.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.10.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.10.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.10.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.10.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.10.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.10.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.10.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.10.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.10.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.10.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.10.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.10.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.10.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.10.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.10.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.10.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.10.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.10.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.10.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.10.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.10.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.10.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.10.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.10.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.10.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.10.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.10.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.10.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.10.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.11.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.11.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.11.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.11.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.11.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.11.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.11.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.11.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.11.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.11.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.11.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.11.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.11.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.11.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.11.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.11.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.11.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.11.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.11.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.11.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.11.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.11.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.11.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.11.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.11.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.11.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.11.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.11.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.11.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.11.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.11.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.11.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.11.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.11.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.11.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.12.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.12.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.12.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.12.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.12.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.12.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.12.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.12.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.12.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.12.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.12.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.12.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.12.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.12.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.12.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.12.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.12.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.12.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.12.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.12.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.12.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.12.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.12.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.12.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.12.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.12.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.12.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.12.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.12.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.12.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.12.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.12.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.12.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.12.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.12.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.13.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.13.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.13.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.13.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.13.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.13.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.13.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.13.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.13.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.13.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.13.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.13.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.13.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.13.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.13.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.13.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.13.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.13.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.13.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.13.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.13.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.13.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.13.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.13.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.13.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.13.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.13.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.13.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.13.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.13.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.13.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.13.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.13.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.13.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.13.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.14.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.14.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.14.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.14.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.14.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.14.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.14.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.14.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.14.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.14.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.14.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.14.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.14.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.14.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.14.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.14.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.14.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.14.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.14.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.14.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.14.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.14.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.14.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.14.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.14.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.14.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.14.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.14.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.14.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.14.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.14.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.14.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.14.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.14.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.14.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.15.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.15.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.15.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.15.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.15.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.15.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.15.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.15.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.15.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.15.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.15.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.15.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.15.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.15.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.15.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.15.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.15.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.15.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.15.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.15.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.15.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.15.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.15.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.15.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.15.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.15.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.15.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.15.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.15.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.15.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.15.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.15.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.15.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.15.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.15.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.16.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.16.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.16.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.16.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.16.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.16.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.16.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.16.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.16.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.16.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.16.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.16.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.16.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.16.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.16.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.16.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.16.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.16.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.16.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.16.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.16.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.16.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.16.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.16.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.16.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.16.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.16.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.16.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.16.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.16.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.16.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.16.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.16.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.16.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.16.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.17.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.17.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.17.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.17.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.17.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.17.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.17.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.17.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.17.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.17.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.17.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.17.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.17.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.17.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.17.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.17.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.17.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.17.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.17.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.17.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.17.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.17.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.17.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.17.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.17.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.17.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.17.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.17.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.17.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.17.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.17.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.17.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.17.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.17.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.17.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.18.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.18.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.18.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.18.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.18.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.18.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.18.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.18.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.18.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.18.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.18.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.18.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.18.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.18.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.18.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.18.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.18.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.18.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.18.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.18.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.18.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.18.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.18.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.18.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.18.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.18.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.18.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.18.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.18.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.18.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.18.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.18.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.18.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.18.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.18.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.19.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.19.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.19.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.19.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.19.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.19.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.19.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.19.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.19.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.19.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.19.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.19.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.19.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.19.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.19.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.19.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.19.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.19.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.19.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.19.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.19.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.19.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.19.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.19.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.19.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.19.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.19.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.19.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.19.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.19.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.19.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.19.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.19.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.19.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.19.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.20.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.20.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.20.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.20.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.20.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.20.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.20.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.20.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.20.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.20.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.20.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.20.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.20.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.20.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.20.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.20.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.20.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.20.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.20.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.20.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.20.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.20.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.20.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.20.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.20.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.20.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.20.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.20.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.20.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.20.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.20.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.20.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.20.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.20.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.20.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.21.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.21.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.21.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.21.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.21.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.21.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.21.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.21.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.21.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.21.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.21.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.21.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.21.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.21.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.21.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.21.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.21.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.21.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.21.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.21.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.21.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.21.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.21.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.21.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.21.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.21.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.21.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.21.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.21.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.21.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.21.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.21.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.21.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.21.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.21.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight.absmax', 'text_decoder.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight.quant_map', 'text_decoder.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight.absmax', 'text_decoder.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight.quant_map', 'text_decoder.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight.absmax', 'text_decoder.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight.quant_map', 'text_decoder.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight.absmax', 'text_decoder.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight.quant_map', 'text_decoder.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.22.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.22.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.22.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.22.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.22.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.22.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.22.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.22.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.22.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.22.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.22.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.22.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.22.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.22.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.22.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight.absmax', 'text_decoder.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight.quant_map', 'text_decoder.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight.absmax', 'text_decoder.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight.quant_map', 'text_decoder.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight.absmax', 'text_decoder.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight.quant_map', 'text_decoder.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight.absmax', 'text_decoder.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight.quant_map', 'text_decoder.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.23.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.23.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.23.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.23.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.23.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.23.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.23.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.23.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.23.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.23.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.23.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.23.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.23.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.23.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.23.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.24.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.24.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.24.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.24.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.24.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.24.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.24.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.24.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.24.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.24.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.24.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.24.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.24.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.24.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.24.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.24.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.24.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.24.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.24.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.24.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.24.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.24.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.24.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.24.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.24.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.24.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.24.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.24.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.24.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.24.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.24.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.24.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.24.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.24.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.24.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.25.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.25.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.25.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.25.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.25.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.25.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.25.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.25.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.25.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.25.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.25.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.25.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.25.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.25.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.25.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.25.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.25.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.25.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.25.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.25.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.25.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.25.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.25.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.25.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.25.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.25.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.25.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.25.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.25.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.25.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.25.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.25.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.25.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.25.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.25.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.26.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.26.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.26.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.26.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.26.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.26.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.26.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.26.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.26.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.26.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.26.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.26.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.26.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.26.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.26.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.26.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.26.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.26.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.26.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.26.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.26.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.26.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.26.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.26.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.26.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.26.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.26.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.26.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.26.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.26.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.26.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.26.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.26.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.26.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.26.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.27.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.27.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.27.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.27.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.27.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.27.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.27.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.27.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.27.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.27.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.27.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.27.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.27.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.27.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.27.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.27.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.27.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.27.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.27.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.27.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.27.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.27.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.27.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.27.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.27.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.27.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.27.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.27.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.27.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.27.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.27.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.27.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.27.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.27.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.27.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.28.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.28.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.28.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.28.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.28.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.28.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.28.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.28.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.28.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.28.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.28.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.28.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.28.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.28.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.28.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.28.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.28.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.28.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.28.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.28.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.28.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.28.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.28.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.28.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.28.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.28.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.28.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.28.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.28.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.28.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.28.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.28.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.28.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.28.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.28.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.29.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.29.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.29.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.29.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.29.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.29.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.29.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.29.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.29.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.29.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.29.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.29.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.29.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.29.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.29.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.29.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.29.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.29.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.29.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.29.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.29.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.29.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.29.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.29.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.29.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.29.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.29.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.29.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.29.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.29.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.29.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.29.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.29.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.29.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.29.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.30.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.30.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.30.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.30.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.30.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.30.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.30.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.30.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.30.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.30.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.30.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.30.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.30.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.30.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.30.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.30.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.30.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.30.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.30.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.30.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.30.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.30.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.30.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.30.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.30.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.30.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.30.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.30.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.30.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.30.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.30.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.30.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.30.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.30.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.30.mlp.down_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.31.self_attn.q_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.31.self_attn.q_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.31.self_attn.q_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.31.self_attn.q_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.31.self_attn.q_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.31.self_attn.k_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.31.self_attn.k_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.31.self_attn.k_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.31.self_attn.k_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.31.self_attn.k_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.31.self_attn.v_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.31.self_attn.v_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.31.self_attn.v_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.31.self_attn.v_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.31.self_attn.v_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.31.self_attn.o_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.31.self_attn.o_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.31.self_attn.o_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.31.self_attn.o_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.31.self_attn.o_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.31.mlp.gate_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.31.mlp.gate_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.31.mlp.gate_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.31.mlp.gate_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.31.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.31.mlp.up_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.31.mlp.up_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.31.mlp.up_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.31.mlp.up_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.31.mlp.up_proj.weight.quant_state.bitsandbytes__nf4', 'text_decoder.base_model.model.model.layers.31.mlp.down_proj.weight.absmax', 'text_decoder.base_model.model.model.layers.31.mlp.down_proj.weight.quant_map', 'text_decoder.base_model.model.model.layers.31.mlp.down_proj.weight.nested_absmax', 'text_decoder.base_model.model.model.layers.31.mlp.down_proj.weight.nested_quant_map', 'text_decoder.base_model.model.model.layers.31.mlp.down_proj.weight.quant_state.bitsandbytes__nf4'])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import gc\n","from tqdm import tqdm\n","\n","test_predictions = []\n","test_references = []\n","tokenizer=mistral_tokenizer\n","model.eval()\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc=\"Inference on Test\"):\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        pixel_values_front = batch[\"pixel_values_front\"].to(device)\n","        pixel_values_lat   = batch[\"pixel_values_lat\"].to(device)\n","\n","        generated_ids = model(\n","            pixel_values_front=pixel_values_front,\n","            pixel_values_lat=pixel_values_lat\n","        )\n","\n","        preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","\n","        test_predictions.extend(preds)\n","\n","        if \"references\" in batch:\n","            refs = batch[\"references\"]\n","            test_references.extend(refs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUXEeSqVuU1y","executionInfo":{"status":"ok","timestamp":1739459595214,"user_tz":300,"elapsed":1860637,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"85fac456-2b19-49df-ecaf-bc47abf43977"},"id":"XUXEeSqVuU1y","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Inference on Test: 100%|██████████| 64/64 [31:00<00:00, 29.07s/it]\n"]}]},{"cell_type":"code","source":["if len(test_references) == len(test_predictions):\n","    bleu4, rouge_l, cider_score = calculate_metrics(test_predictions, test_references)\n","    print(f\"Test BLEU-4: {bleu4:.4f} | Test ROUGE-L: {rouge_l:.4f} | Test CIDEr: {cider_score:.4f}\")\n","else:\n","    print(\"No references in test set, skipping metrics computation.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkgqDqkZwYcx","executionInfo":{"status":"ok","timestamp":1739459600200,"user_tz":300,"elapsed":4988,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"f1b5a90c-f1fc-45ea-dbf0-d31fc9781bbf"},"id":"zkgqDqkZwYcx","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test BLEU-4: 0.0394 | Test ROUGE-L: 0.1410 | Test CIDEr: 0.0000\n"]}]},{"cell_type":"code","source":["for i in range(2):\n","    print(f\"--- Test Sample {i} ---\")\n","    print(f\"Generated: {test_predictions[i]}\")\n","    if test_references:\n","        print(f\"Reference: {test_references[i]}\")\n","    print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P1exnzoxwwSX","executionInfo":{"status":"ok","timestamp":1739459668148,"user_tz":300,"elapsed":92,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"a535043b-fce4-45ff-e7b6-569ea01acc76"},"id":"P1exnzoxwwSX","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Test Sample 0 ---\n","Generated: the heart is normal in size. the mediastinal contours are within normal limits. there is no pleural effusion or pneumothorax. there is no focal airspace consolidation. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is no evidence of acute cardiopulmonary disease. there is\n","Reference: there are diffuse bilateral interstitial and alveolar opacities consistent with chronic obstructive lung disease and bullous emphysema. there are irregular opacities in the left lung apex, that could represent a cavitary lesion in the left lung apex.there are streaky opacities in the right upper lobe, xxxx scarring. the cardiomediastinal silhouette is normal in size and contour. there is no pneumothorax or large pleural effusion. 1. bullous emphysema and interstitial fibrosis. 2. probably scarring in the left apex, although difficult to exclude a cavitary lesion. 3. opacities in the bilateral upper lobes could represent scarring, however the absence of comparison exam, recommend short interval followup radiograph or ct thorax to document resolution.\n","\n","--- Test Sample 1 ---\n","Generated: the heart is normal in size. the mediastinal contours are within normal limits. the lungs are clear and free of any focal airspace disease. there is no pleural effusion or pneumothorax. there is no acute bony abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there is no acute cardiopulmonary abnormality. there\n","Reference: the cardiomediastinal silhouette and pulmonary vasculature are within normal limits. there is no pneumothorax or pleural effusion. there are no focal areas of consolidation. cholecystectomy clips are present. small t-spine osteophytes. there is biapical pleural thickening, unchanged from prior. mildly hyperexpanded lungs. no acute cardiopulmonary abnormality.\n","\n"]}]},{"cell_type":"code","source":["optimizer = bnb.optim.Adam8bit(\n","    [\n","        {'params': trainable_proj,    'lr': 1e-3},\n","        {'params': trainable_decoder, 'lr': 1e-3, 'weight_decay': 0.01},\n","        {'params': trainable_encoder, 'lr': 1e-3}\n","    ],\n","    betas=(0.9, 0.999),\n","    optim_bits=8\n",")"],"metadata":{"id":"fxNnIrTPw065"},"id":"fxNnIrTPw065","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_validate(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=valid_loader,\n","    optimizer=optimizer,\n","    tokenizer=medalpaca_tokenizer,\n","    num_epochs=50,\n","    device=device,\n","    save_path=\"/content/drive/MyDrive/Small_human_extracted/best_cxr_model_2.pt\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":778},"id":"351VXwbsxuh6","executionInfo":{"status":"error","timestamp":1739464125193,"user_tz":300,"elapsed":589489,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"d35a2047-6d19-4cef-a7dc-30413ae0028f"},"id":"351VXwbsxuh6","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Training Epoch 1/50:   0%|          | 0/224 [01:12<?, ?it/s, Train Loss=0.4095]"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1/50 - Step 50/224] Train Loss: 0.4095\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1/50:   0%|          | 0/224 [02:23<?, ?it/s, Train Loss=0.3583]"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1/50 - Step 100/224] Train Loss: 0.3583\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1/50:   0%|          | 0/224 [03:35<?, ?it/s, Train Loss=0.5014]"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1/50 - Step 150/224] Train Loss: 0.5014\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1/50:   0%|          | 0/224 [04:47<?, ?it/s, Train Loss=0.4774]"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1/50 - Step 200/224] Train Loss: 0.4774\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1/50:   0%|          | 0/224 [05:21<?, ?it/s, Train Loss=0.6585]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50 - Average Train Loss: 0.6533\n"]},{"output_type":"stream","name":"stderr","text":["\n","Validation Epoch 1/50:   0%|          | 0/32 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n","  warnings.warn(\n","\n","Validation Epoch 1/50:   0%|          | 0/32 [00:30<?, ?it/s, Val Loss=0.2177]\u001b[A\n","Validation Epoch 1/50:   0%|          | 0/32 [00:58<?, ?it/s, Val Loss=0.2888]\u001b[A\n","Validation Epoch 1/50:   0%|          | 0/32 [01:26<?, ?it/s, Val Loss=0.3020]\u001b[A\n","Validation Epoch 1/50:   0%|          | 0/32 [01:54<?, ?it/s, Val Loss=0.3098]\u001b[A\n","Validation Epoch 1/50:   0%|          | 0/32 [02:25<?, ?it/s, Val Loss=0.3222]\u001b[A\n","Validation Epoch 1/50:   0%|          | 0/32 [02:54<?, ?it/s, Val Loss=0.3114]\u001b[A\n","Validation Epoch 1/50:   0%|          | 0/32 [03:20<?, ?it/s, Val Loss=0.3934]\u001b[A\n","Validation Epoch 1/50:   0%|          | 0/32 [03:49<?, ?it/s, Val Loss=0.2823]\u001b[A\n","Validation Epoch 1/50:   0%|          | 0/32 [04:18<?, ?it/s, Val Loss=0.3626]\u001b[A"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-5fa0c5e4bdcd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_and_validate(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-12ee451aa00a>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, optimizer, tokenizer, num_epochs, device, save_path)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mtotal_val_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 generated_ids = model(\n\u001b[0m\u001b[1;32m    117\u001b[0m                     \u001b[0mpixel_values_front\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values_front\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mpixel_values_lat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values_lat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-c6e4fc7c164a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values_front, pixel_values_lat, input_ids, attention_mask, labels, max_new_tokens)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mvision_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojected_vision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 1, 4096)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             generated = self.text_decoder.generate(\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvision_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    835\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         causal_mask = self._update_causal_mask(\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m_update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sdpa\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0musing_static_cache\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n\u001b[0m\u001b[1;32m    645\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_ignore_causal_mask_sdpa\u001b[0;34m(attention_mask, inputs_embeds, past_key_values_length, sliding_window, is_training)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mquery_length\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey_value_length\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mquery_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                     \u001b[0;31m# For query_length == 1, causal attention and bi-directional attention are the same.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Xxjyo00jxySv"},"id":"Xxjyo00jxySv","execution_count":null,"outputs":[]}]}