{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"MI7OpdEc_a6U","executionInfo":{"status":"ok","timestamp":1738606176297,"user_tz":300,"elapsed":11866,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import h5py\n","import time\n","from tqdm import tqdm\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import re\n","from collections import Counter\n","from torch.nn.utils.rnn import pad_sequence\n","from torchvision.models import densenet121\n","from rouge_score import rouge_scorer\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from nltk.translate.bleu_score import sentence_bleu\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import os\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofNIIZMGAN86","executionInfo":{"status":"ok","timestamp":1738606232784,"user_tz":300,"elapsed":52559,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"3f8d137a-aaa8-487a-b6bb-3f9520282a71"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKLA42-m_a6V","executionInfo":{"status":"ok","timestamp":1738606467040,"user_tz":300,"elapsed":5005,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"a54e8d9a-fd44-41a2-8849-e1eb25c99731"},"outputs":[{"output_type":"stream","name":"stdout","text":["Weights loaded successfully into PyTorch DenseNet-121 model.\n"]}],"source":["# Load Pretrained DenseNet-121 from PyTorch\n","densenet = models.densenet121(weights=None)\n","state_dict = densenet.state_dict()\n","\n","# Load the Keras Weights from .h5 File\n","keras_weights_path = '/content/drive/MyDrive/brucechou1983_CheXNet_Keras_0.3.0_weights.h5'\n","with h5py.File(keras_weights_path, 'r') as f:\n","    keras_weights = {k: np.array(v) for k, v in f.items()}\n","\n","# Map Keras Weights to PyTorch Model\n","def load_weights(state_dict, keras_weights):\n","    for key in keras_weights.keys():\n","        if 'kernel' in key or 'bias' in key:  # Identify weight types\n","            pytorch_key = key.replace('kernel', 'weight').replace('bias', 'bias')\n","            pytorch_key = pytorch_key.replace('/', '.')  # Adjust naming format\n","\n","            if pytorch_key in state_dict:\n","                weight = torch.tensor(keras_weights[key])\n","                if len(weight.shape) == 4:  # For Conv weights\n","                    weight = weight.permute(3, 2, 0, 1)\n","                state_dict[pytorch_key] = weight\n","    return state_dict\n","\n","# Apply Weights to PyTorch DenseNet\n","state_dict = load_weights(state_dict, keras_weights)\n","densenet.load_state_dict(state_dict)\n","\n","print(\"Weights loaded successfully into PyTorch DenseNet-121 model.\")\n","\n","# Step 4: Use DenseNet-121 as a Feature Extractor\n","class CheXNetFeatureExtractor(nn.Module):\n","    def __init__(self, densenet):\n","        super(CheXNetFeatureExtractor, self).__init__()\n","        self.features = densenet.features\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.global_avg_pool(x)\n","        x = torch.flatten(x, 1)\n","        return x\n","\n","# Instantiate Feature Extractor\n","chexnet = CheXNetFeatureExtractor(densenet)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vMMFzI3e_a6W","executionInfo":{"status":"ok","timestamp":1738606482689,"user_tz":300,"elapsed":1270,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"086d1a04-9b7f-4bea-d7b9-cd3cb888ebaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Vector Shape: torch.Size([1, 1024])\n"]}],"source":["# Testing of loaded model\n","input_tensor = torch.randn(1, 3, 224, 224)\n","output = chexnet(input_tensor)\n","print(\"Feature Vector Shape:\", output.shape)  # Should be [1, 1024]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RrdBFmaw_a6W","executionInfo":{"status":"ok","timestamp":1738606534613,"user_tz":300,"elapsed":2818,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"7f922518-fa30-4d96-833e-8f5a23d697d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary Size: 1461\n"]}],"source":["# Load Dataset\n","train_dataset = pd.read_csv('/content/drive/MyDrive/Final_Train_Data.csv')\n","test_dataset = pd.read_csv('/content/drive/MyDrive/Final_Test_Data.csv')\n","cv_dataset = pd.read_csv('/content/drive/MyDrive/Final_CV_Data.csv')\n","\n","# Tokenize and Clean Text\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n","    return text\n","\n","# Build Vocabulary\n","def build_vocab(texts, min_freq=1):\n","    counter = Counter()\n","    for text in texts:\n","        counter.update(clean_text(text).split())\n","\n","    vocab = {\"<PAD>\": 0, \"<START>\": 1, \"<END>\": 2, \"<UNK>\": 3}\n","    for word, freq in counter.items():\n","        if freq >= min_freq:\n","            vocab[word] = len(vocab)\n","    return vocab\n","\n","# Create Vocabulary\n","vocab = build_vocab(train_dataset['Report'].values)\n","vocab_size = len(vocab)\n","print(\"Vocabulary Size:\", vocab_size)\n","\n","# Text to Sequence\n","def text_to_sequence(text, vocab):\n","    tokens = clean_text(text).split()\n","    return [vocab.get(\"<START>\")] + [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens] + [vocab.get(\"<END>\")]\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MRpLYleR_a6X","executionInfo":{"status":"ok","timestamp":1738606540144,"user_tz":300,"elapsed":820,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["import re\n","from collections import Counter\n","\n","class CustomTokenizer:\n","    def __init__(self, min_freq=1, special_tokens=[\"<PAD>\", \"<START>\", \"<END>\", \"<UNK>\"]):\n","        \"\"\"\n","        A custom tokenizer for converting text to token sequences and vice versa.\n","\n","        Args:\n","            min_freq (int): Minimum frequency for a word to be included in the vocabulary.\n","            special_tokens (list): List of special tokens like <PAD>, <START>, etc.\n","        \"\"\"\n","        self.word_index = {}  # Word to index mapping\n","        self.index_word = {}  # Index to word mapping\n","        self.min_freq = min_freq\n","        self.special_tokens = special_tokens\n","        self.word_counter = Counter()\n","\n","        # Initialize special tokens\n","        for idx, token in enumerate(self.special_tokens):\n","            self.word_index[token] = idx\n","            self.index_word[idx] = token\n","\n","    def fit_on_texts(self, texts):\n","        \"\"\"\n","        Fit the tokenizer on a list of text data to build the vocabulary.\n","\n","        Args:\n","            texts (list of str): List of text sequences.\n","        \"\"\"\n","        # Count word frequencies\n","        for text in texts:\n","            tokens = self._clean_and_tokenize(text)\n","            self.word_counter.update(tokens)\n","\n","        # Add words to the vocabulary if they meet the min_freq criteria\n","        for word, freq in self.word_counter.items():\n","            if freq >= self.min_freq and word not in self.word_index:\n","                idx = len(self.word_index)\n","                self.word_index[word] = idx\n","                self.index_word[idx] = word\n","\n","    def texts_to_sequences(self, texts):\n","        \"\"\"\n","        Convert a list of texts to sequences of word indices.\n","\n","        Args:\n","            texts (list of str): List of text sequences.\n","\n","        Returns:\n","            list of list of int: List of token index sequences.\n","        \"\"\"\n","        sequences = []\n","        for text in texts:\n","            tokens = self._clean_and_tokenize(text)\n","            sequence = [self.word_index.get(token, self.word_index[\"<UNK>\"]) for token in tokens]\n","            sequences.append(sequence)\n","        return sequences\n","\n","    def sequences_to_texts(self, sequences):\n","        \"\"\"\n","        Convert a list of sequences of word indices back to text.\n","\n","        Args:\n","            sequences (list of list of int): List of token index sequences.\n","\n","        Returns:\n","            list of str: List of text sequences.\n","        \"\"\"\n","        texts = []\n","        for seq in sequences:\n","            text = \" \".join([self.index_word.get(idx, \"<UNK>\") for idx in seq if idx != self.word_index[\"<PAD>\"]])\n","            texts.append(text)\n","        return texts\n","\n","    def _clean_and_tokenize(self, text):\n","        \"\"\"\n","        Clean and tokenize text into a list of words.\n","\n","        Args:\n","            text (str): Input text string.\n","\n","        Returns:\n","            list of str: List of tokens.\n","        \"\"\"\n","        text = text.lower()\n","        text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # Remove special characters\n","        return text.split()\n","\n","    def __call__(self, text):\n","        \"\"\"\n","        Makes the tokenizer callable for a single text input.\n","\n","        Args:\n","            text (str): A single text sequence.\n","\n","        Returns:\n","            list of int: Token index sequence.\n","        \"\"\"\n","        tokens = self._clean_and_tokenize(text)\n","        return [self.word_index.get(token, self.word_index[\"<UNK>\"]) for token in tokens]"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"314pMCzU_a6X","executionInfo":{"status":"ok","timestamp":1738606554496,"user_tz":300,"elapsed":1051,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["# Initialize and Fit Tokenizer\n","tokenizer = CustomTokenizer()\n","\n","# Concatenate all reports into a single list\n","all_reports = pd.concat([\n","    train_dataset['Report'],\n","    cv_dataset['Report'],\n","    test_dataset['Report']\n","]).values.tolist()\n","\n","# Fit the tokenizer on all reports\n","tokenizer.fit_on_texts(all_reports)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tz80YP3b_a6X","executionInfo":{"status":"ok","timestamp":1738606558721,"user_tz":300,"elapsed":1076,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"3356c193-8d6b-4f40-e6f7-880bf9232f47"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1599"]},"metadata":{},"execution_count":12}],"source":["vocab_size = len(tokenizer.word_index.keys()) + 1\n","vocab_size"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"pqrzf8au_a6Y","executionInfo":{"status":"ok","timestamp":1738606560596,"user_tz":300,"elapsed":2,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"eG_rW01J_a6Y"},"source":["## Generate Embeddings using  GloVe Vectors"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v01CJ6gX_a6Y","executionInfo":{"status":"ok","timestamp":1738606807725,"user_tz":300,"elapsed":206046,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"78b9b58a-5cef-4ddc-bfe4-2e18112f13ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-03 18:16:41--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2025-02-03 18:16:41--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2025-02-03 18:16:42--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n","\n","2025-02-03 18:19:22 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n","Loaded 400000 words from GloVe.\n"]}],"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip\n","\n","glove_path = 'glove.6B.300d.txt'\n","\n","# Load GloVe Vectors into a Dictionary\n","def load_glove_vectors(glove_path):\n","    glove_dict = {}\n","    with open(glove_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.strip().split()\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            glove_dict[word] = vector\n","    return glove_dict\n","\n","# Load GloVe\n","glove_vectors = load_glove_vectors(glove_path)\n","print(f\"Loaded {len(glove_vectors)} words from GloVe.\")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"GT8gkdx9_a6Y","executionInfo":{"status":"ok","timestamp":1738606808657,"user_tz":300,"elapsed":2,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["embedding_matrix = np.zeros((vocab_size,300))\n","for word, i in tokenizer.word_index.items():\n","    if word in glove_vectors.keys():\n","        vec = glove_vectors[word]\n","        embedding_matrix[i] = vec\n","    else:\n","        continue"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"mCWyrZ2d_a6Z","executionInfo":{"status":"ok","timestamp":1738606812513,"user_tz":300,"elapsed":958,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["class CheXNetReportDataset(Dataset):\n","    def __init__(self, csv_file, root_dir, chexnet, tokenizer, transform=None, max_length=153):\n","        \"\"\"\n","        Args:\n","            csv_file (str): Path to the CSV file containing Person_id and Report.\n","            root_dir (str): Root directory containing the image files.\n","            chexnet (CheXNetFeatureExtractor): Pretrained CheXNet feature extractor.\n","            tokenizer (CustomTokenizer): Tokenizer for converting text to sequences.\n","            transform: Image preprocessing transforms.\n","            max_length (int): Maximum length of tokenized reports (padded/truncated).\n","        \"\"\"\n","        self.data = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.chexnet = chexnet\n","        self.tokenizer = tokenizer\n","        self.transform = transform\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def pad_sequence(self, sequence):\n","        \"\"\"Pad or truncate a tokenized sequence to the maximum length.\"\"\"\n","        if len(sequence) < self.max_length:\n","            sequence += [0] * (self.max_length - len(sequence))  # Pad with 0 (e.g., <PAD>)\n","        else:\n","            sequence = sequence[:self.max_length]\n","        return sequence\n","\n","    def load_image(self, image_path):\n","        \"\"\"Load and preprocess an image.\"\"\"\n","        image = Image.open(image_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","    def __getitem__(self, idx):\n","        person_id = self.data.iloc[idx][\"Person_id\"]\n","        report = self.data.iloc[idx][\"Report\"]\n","        image_path = os.path.join(self.root_dir, self.data.iloc[idx][\"Image1\"])\n","        image = self.load_image(image_path)\n","\n","        # Extract features using CheXNet\n","        with torch.no_grad():\n","            img_feature = self.chexnet(image.unsqueeze(0)).squeeze(0)\n","\n","        # Tokenize and pad the report\n","        tokenized_report = self.tokenizer(report)\n","        padded_report = self.pad_sequence(tokenized_report)\n","\n","        # Calculate caption lengths\n","        length = min(len(tokenized_report), self.max_length)\n","\n","        return img_feature, torch.tensor(padded_report, dtype=torch.long), torch.tensor(tokenized_report, dtype=torch.long), length\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q4uTTVdY_a6Z","executionInfo":{"status":"ok","timestamp":1738606897423,"user_tz":300,"elapsed":292,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"7c44c71b-b94e-4f07-cd57-479c39276ced"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples in the training dataset: 2764\n","Number of samples in the validation dataset: 560\n","Number of samples in the test dataset: 383\n","Number of batches in the training DataLoader: 87\n","Number of batches in the validation DataLoader: 18\n","Number of batches in the test DataLoader: 12\n"]}],"source":["BATCH_SIZE=32\n","EPOCHS = 10\n","\n","# Image preprocessing transforms for CheXNet feature extraction\n","image_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# DataLoader Function\n","def create_dataloader(dataset, batch_size, shuffle=True):\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2,pin_memory=True)\n","\n","root_dir = \"/content/drive/MyDrive/\"\n","# Assuming chexnet is a preloaded dictionary and tokenizer is initialized\n","dataset_train = CheXNetReportDataset(\"/content/drive/MyDrive/Final_Train_Data.csv\",root_dir,chexnet=chexnet,tokenizer=tokenizer,transform=image_transform)\n","dataset_test = CheXNetReportDataset(\"/content/drive/MyDrive/Final_Test_Data.csv\",root_dir,chexnet=chexnet,tokenizer=tokenizer,transform=image_transform)\n","dataset_valid = CheXNetReportDataset(\"/content/drive/MyDrive/Final_CV_Data.csv\",root_dir,chexnet=chexnet,tokenizer=tokenizer,transform=image_transform)\n","\n","data_loader_train = create_dataloader(dataset_train, BATCH_SIZE)\n","data_loader_test = create_dataloader(dataset_test, BATCH_SIZE)\n","data_loader_valid = create_dataloader(dataset_valid, BATCH_SIZE)\n","\n","# Print number of samples in each dataset\n","print(f\"Number of samples in the training dataset: {len(dataset_train)}\")\n","print(f\"Number of samples in the validation dataset: {len(dataset_valid)}\")\n","print(f\"Number of samples in the test dataset: {len(dataset_test)}\")\n","\n","# Print number of batches in each DataLoader\n","print(f\"Number of batches in the training DataLoader: {len(data_loader_train)}\")\n","print(f\"Number of batches in the validation DataLoader: {len(data_loader_valid)}\")\n","print(f\"Number of batches in the test DataLoader: {len(data_loader_test)}\")\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"qza4Re0J_a6Z","executionInfo":{"status":"ok","timestamp":1738606876412,"user_tz":300,"elapsed":529,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"c05fac70-d033-47a5-ebc5-c868388284b0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'encoder_decoder_epoch_final_20250203_182115.pth'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}],"source":["from datetime import datetime\n","# Get the current timestamp\n","current_timestamp = datetime.now()\n","# Format the timestamp without spaces or dashes (e.g., YYYYMMDD_HHMMSS)\n","formatted_timestamp = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n","formatted_timestamp\n","\n","model_name = f'encoder_decoder_epoch_final_{formatted_timestamp}.pth'\n","model_name"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"rPoLlsHv_a6Z","executionInfo":{"status":"ok","timestamp":1738606877764,"user_tz":300,"elapsed":1,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["max_seq_len = 153"]},{"cell_type":"markdown","metadata":{"id":"SRQuf0Xc_a6Z"},"source":["## Model"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"DcJh4HIi_a6Z","executionInfo":{"status":"ok","timestamp":1738607390914,"user_tz":300,"elapsed":307,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        \"\"\"\n","        Args:\n","            encoder_dim: Feature dimension of the encoder (image branch).\n","            decoder_dim: Hidden dimension of the decoder (LSTM hidden state).\n","            attention_dim: Dimension of the attention layer.\n","        \"\"\"\n","        super(Attention, self).__init__()\n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # Encoder projection\n","        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # Decoder projection\n","        self.full_att = nn.Linear(attention_dim, 1)  # Scalar attention score\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)  # Normalize scores\n","\n","    def forward(self, encoder_out, decoder_hidden):\n","        \"\"\"\n","        Forward pass for attention mechanism.\n","        Args:\n","            encoder_out: Features from the encoder (batch_size, seq_len, encoder_dim).\n","            decoder_hidden: Hidden state from the decoder (batch_size, decoder_dim).\n","        Returns:\n","            context: Weighted sum of encoder outputs (batch_size, encoder_dim).\n","            alpha: Attention weights (batch_size, seq_len).\n","        \"\"\"\n","        att1 = self.encoder_att(encoder_out)  # (batch_size, seq_len, attention_dim)\n","        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # (batch_size, 1, attention_dim)\n","        att = self.full_att(self.relu(att1 + att2)).squeeze(2)  # (batch_size, seq_len)\n","        alpha = self.softmax(att)  # Attention weights (batch_size, seq_len)\n","        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # Context vector (batch_size, encoder_dim)\n","        return context, alpha\n","\n","class DecoderWithAttention(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, encoder_dim, decoder_dim, attention_dim):\n","        \"\"\"\n","        Decoder with attention for image captioning.\n","        Args:\n","            vocab_size: Size of the vocabulary.\n","            embedding_dim: Dimension of word embeddings.\n","            encoder_dim: Dimension of encoder output features.\n","            decoder_dim: Dimension of LSTM hidden state.\n","            attention_dim: Dimension of the attention layer.\n","        \"\"\"\n","        super(DecoderWithAttention, self).__init__()\n","        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # Attention module\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n","        self.lstm = nn.LSTMCell(embedding_dim + encoder_dim, decoder_dim)  # LSTMCell with context\n","        self.fc = nn.Linear(decoder_dim, vocab_size)  # Output layer\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # Initialize LSTM hidden state\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # Initialize LSTM cell state\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, encoder_out, captions, lengths):\n","        \"\"\"\n","        Forward pass for the decoder with attention.\n","        Args:\n","            encoder_out: Encoder features (batch_size, seq_len, encoder_dim).\n","            captions: Ground truth captions (batch_size, max_seq_len).\n","            lengths: Caption lengths for each sample in the batch.\n","        Returns:\n","            predictions: Vocabulary scores (batch_size, max_seq_len, vocab_size).\n","            alphas: Attention weights (batch_size, max_seq_len, seq_len).\n","        \"\"\"\n","        batch_size = encoder_out.size(0)\n","        seq_len = encoder_out.size(1)\n","        encoder_dim = encoder_out.size(2)\n","\n","        # Initialize LSTM states\n","        h = self.init_h(encoder_out.mean(dim=1))  # (batch_size, decoder_dim)\n","        c = self.init_c(encoder_out.mean(dim=1))  # (batch_size, decoder_dim)\n","\n","        # Embed captions\n","        embeddings = self.embedding(captions)  # (batch_size, max_seq_len, embedding_dim)\n","\n","        predictions = torch.zeros(batch_size, max(lengths), self.fc.out_features).to(encoder_out.device)\n","        alphas = torch.zeros(batch_size, max(lengths), seq_len).to(encoder_out.device)\n","\n","        for t in range(max(lengths)):\n","            context, alpha = self.attention(encoder_out, h)  # Compute attention\n","            lstm_input = torch.cat([embeddings[:, t, :], context], dim=1)  # Concatenate embedding and context\n","            h, c = self.lstm(lstm_input, (h, c))  # LSTM step\n","            preds = self.fc(self.dropout(h))  # Compute predictions\n","            predictions[:, t, :] = preds\n","            alphas[:, t, :] = alpha  # Store attention weights\n","\n","        return predictions, alphas\n","\n","class MultiModalModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim, image_input_dim, attention_dim, output_dim):\n","        super(MultiModalModel, self).__init__()\n","\n","        # Text Input Branch\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = False  # Make embeddings non-trainable\n","\n","        # Project text embeddings to hidden_dim\n","        self.text_fc = nn.Linear(embedding_dim, hidden_dim)\n","\n","        # ✅ Fix: Ensure `image_fc` matches `image_input_dim`\n","        self.image_fc = nn.Linear(image_input_dim, hidden_dim)  # Ensure `image_input_dim` is correct\n","\n","        # Decoder with Attention\n","        self.decoder = DecoderWithAttention(\n","            vocab_size=vocab_size,\n","            embedding_dim=hidden_dim,  # Now using hidden_dim for consistency\n","            encoder_dim=hidden_dim,\n","            decoder_dim=hidden_dim,\n","            attention_dim=attention_dim,\n","        )\n","\n","    def forward(self, text_input, image_input, captions, lengths):\n","        # Text Processing Path (Encoder)\n","        embedded = self.text_fc(self.embedding(text_input))  # [B, seq_len, hidden_dim]\n","\n","        # ✅ Fix: Ensure image_input is correctly reshaped\n","        if image_input.dim() == 4:  # If image_input has extra batch dimension\n","            image_input = image_input.view(image_input.size(0), -1)\n","\n","        image_features = torch.relu(self.image_fc(image_input))  # [B, hidden_dim]\n","        image_features = image_features.unsqueeze(1)  # Add sequence dimension: [B, 1, hidden_dim]\n","\n","        # Combine Image and Text Features\n","        encoder_out = torch.cat([embedded, image_features], dim=1)  # [B, seq_len + 1, hidden_dim]\n","\n","        # Decoder with Attention\n","        predictions, alphas = self.decoder(encoder_out, captions, lengths)\n","        return predictions, alphas\n"]},{"cell_type":"markdown","metadata":{"id":"XxDIyhAC_a6Z"},"source":["## Training Loop"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"q1F9GVhd_a6a","executionInfo":{"status":"ok","timestamp":1738606906815,"user_tz":300,"elapsed":444,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["def train_model(\n","    model, train_dataloader, val_dataloader, optimizer, criterion, device, epochs, save_path='best_model.pth'\n","):\n","    epoch_train_loss = []\n","    epoch_val_loss = []\n","    epoch_train_acc = []\n","    epoch_val_acc = []\n","    best_val_loss = float('inf')\n","\n","    model.to(device)\n","    for epoch in range(epochs):\n","        print(f'EPOCH : {epoch + 1}')\n","        start = time.time()\n","\n","        # Training Phase\n","        model.train()\n","        train_loss = 0\n","        correct_train = 0\n","        total_train = 0\n","        with tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\") as train_bar:\n","            for img_features, reports, captions, lengths in train_bar:\n","                img_features = img_features.to(device)\n","                reports = reports.to(device)\n","                captions = captions.to(device)\n","                #lengths = torch.tensor(lengths).to(device)\n","                lengths = lengths.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                # Forward pass\n","                outputs, _ = model(reports, img_features, captions, lengths)  # Updated to include captions and lengths\n","                outputs = outputs.view(-1, outputs.size(-1))  # Flatten predictions\n","                reports = reports.view(-1)  # Flatten ground truth\n","\n","                # Compute loss\n","                loss = criterion(outputs, reports)\n","\n","                # Backward pass with gradient clipping\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n","                optimizer.step()\n","\n","                train_loss += loss.item()\n","\n","                # Update accuracy\n","                _, predicted = torch.max(outputs, dim=1)\n","                correct_train += (predicted == reports).sum().item()\n","                total_train += reports.size(0)\n","\n","                # Update tqdm bar\n","                train_bar.set_postfix(loss=loss.item(), acc=correct_train / total_train)\n","\n","        train_loss /= len(train_dataloader)\n","        train_acc = correct_train / total_train\n","        epoch_train_loss.append(train_loss)\n","        epoch_train_acc.append(train_acc)\n","\n","        # Validation Phase\n","        model.eval()\n","        val_loss = 0\n","        correct_val = 0\n","        total_val = 0\n","        with torch.no_grad():\n","            with tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}\") as val_bar:\n","                for img_features, reports, captions, lengths in val_bar:\n","                    img_features = img_features.to(device)\n","                    reports = reports.to(device)\n","                    captions = captions.to(device)\n","\n","                    # Safely handle lengths\n","                    if not isinstance(lengths, torch.Tensor):\n","                        lengths = torch.tensor(lengths, device=device)\n","                    else:\n","                        lengths = lengths.to(device)\n","\n","                    invalid_indices = captions[(captions < 0) | (captions >= vocab_size)]\n","                    # print(f\"Invalid token indices: {invalid_indices}\")\n","                    # print(f\"Captions tensor: {captions}\")\n","                    # print(f\"Vocabulary size: {vocab_size}\")\n","                    # print(f\"Min index: {captions.min()}, Max index: {captions.max()}\")\n","\n","\n","                    # Check indices and lengths\n","                    assert torch.all((captions >= 0) & (captions < vocab_size)), \"Captions contain invalid token indices.\"\n","                    assert captions.size(1) <= max_seq_len, f\"Caption length exceeds max_seq_len: {captions.size(1)} > {max_seq_len}\"\n","\n","                    # Forward pass for validation\n","                    outputs, _ = model(reports, img_features, captions, lengths)\n","\n","                    # Flatten outputs and compute loss\n","                    outputs = outputs.view(-1, outputs.size(-1))\n","                    reports = reports.view(-1)\n","                    loss = criterion(outputs, reports)\n","\n","                    # Update metrics\n","                    val_loss += loss.item()\n","                    _, predicted = torch.max(outputs, dim=1)\n","                    correct_val += (predicted == reports).sum().item()\n","                    total_val += reports.size(0)\n","\n","        val_loss /= len(val_dataloader)\n","        val_acc = correct_val / total_val\n","        epoch_val_loss.append(val_loss)\n","        epoch_val_acc.append(val_acc)\n","\n","        # Save the best model based on validation loss\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), save_path)\n","            print(f\"Best model saved with validation loss: {val_loss:.4f}\")\n","\n","        print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n","        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n","        print(f\"Time Taken for this Epoch: {time.time() - start:.2f} sec\")\n","\n","    return epoch_train_loss, epoch_val_loss, epoch_train_acc, epoch_val_acc\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W5ZU4DXV_a6a","executionInfo":{"status":"ok","timestamp":1738606909441,"user_tz":300,"elapsed":2,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"99bf84c6-b7fa-478f-cac8-80d42da67e1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Updated vocab_size: 1602\n"]}],"source":["vocab_size = len(tokenizer.word_index) + len(tokenizer.special_tokens)  # Include special tokens\n","print(f\"Updated vocab_size: {vocab_size}\")\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koc_pLsP_a6a","executionInfo":{"status":"ok","timestamp":1738606911301,"user_tz":300,"elapsed":3,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"6058a3f2-de8c-437a-bee9-f3e882e397a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding layer size: 1599\n"]}],"source":["print(f\"Embedding layer size: {embedding_matrix.shape[0]}\")  # Should match vocab_size"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"N69BghkG_a6a","executionInfo":{"status":"ok","timestamp":1738606913202,"user_tz":300,"elapsed":320,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["\n","attention_dim = 256      # Set attention dimension\n","embedding_dim = 300      # GloVe embedding dimension\n","hidden_dim = 256         # LSTM hidden state dimension\n","image_input_dim = 1024   # CheXNet feature extractor output dimension\n","output_dim = vocab_size  # Output dimension (vocabulary size)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zoi9eyHv_a6a","executionInfo":{"status":"ok","timestamp":1738606915540,"user_tz":300,"elapsed":764,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"b94a5ed2-5e76-4aee-9dda-3fa685502f98"},"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting embedding matrix from 1599 to 1602.\n"]}],"source":["# Convert embedding_matrix to a PyTorch tensor\n","embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n","\n","# Adjust embedding matrix to match vocab_size\n","if embedding_matrix.shape[0] != vocab_size:\n","    print(f\"Adjusting embedding matrix from {embedding_matrix.shape[0]} to {vocab_size}.\")\n","    adjusted_embedding_matrix = torch.zeros(vocab_size, embedding_dim, dtype=torch.float32)\n","    adjusted_embedding_matrix[:embedding_matrix.shape[0], :] = embedding_matrix  # Copy existing embeddings\n","    embedding_matrix = adjusted_embedding_matrix\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MGi6-VT7_a6a","executionInfo":{"status":"ok","timestamp":1738606917992,"user_tz":300,"elapsed":1,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"2ae50d6f-ac7c-4f78-b288-f5a9221630e2"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-21-d7a4f6cddfb7>:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n"]}],"source":["# Instantiate Model\n","model = MultiModalModel(\n","    vocab_size=vocab_size,\n","    embedding_dim=embedding_dim,\n","    embedding_matrix=embedding_matrix,\n","    hidden_dim=hidden_dim,\n","    image_input_dim=image_input_dim,\n","    attention_dim=attention_dim,\n","    output_dim=output_dim\n",")\n","\n","model = model.to(device)\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"de_YD4gY_a6a","executionInfo":{"status":"error","timestamp":1738607429280,"user_tz":300,"elapsed":34048,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}},"outputId":"84e3f39a-0125-4c56-ecc2-11cdd740af4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH : 1\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1:   0%|          | 0/87 [00:32<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (32x1024 and 224x256)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-5f18be984a50>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_train_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Save Model Weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-db32a0065828>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion, device, epochs, save_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreports\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Updated to include captions and lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mreports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-d7a4f6cddfb7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_input, image_input, captions, lengths)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Image Processing Path (Encoder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add sequence dimension: [B, 1, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x1024 and 224x256)"]}],"source":["# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Train the Model\n","epoch_train_loss, epoch_val_loss, epoch_train_acc, epoch_val_acc = train_model(model, data_loader_train, data_loader_valid, optimizer, criterion, device, EPOCHS,model_name)\n","\n","# Save Model Weights\n","torch.save(model.state_dict(), model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCha12bG_a6b","executionInfo":{"status":"aborted","timestamp":1738607429280,"user_tz":300,"elapsed":4,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["def predict(model, dataloader, tokenizer, device):\n","    \"\"\"\n","    Generates predictions for the input data using the trained model and calculates accuracy.\n","    \"\"\"\n","    model.eval()\n","    predictions = []\n","    targets = []\n","    total_correct = 0\n","    total_tokens = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Generating Predictions\"):\n","            # Unpack batch\n","            img_features, reports, captions, lengths = batch\n","\n","            # Move data to the same device as the model\n","            img_features = img_features.to(device)\n","            reports = reports.to(device)\n","            captions = captions.to(device)\n","            lengths = torch.tensor(lengths).to(device)\n","\n","            # Forward pass\n","            outputs, _ = model(reports, img_features, captions, lengths)  # Adjust call to include captions and lengths\n","\n","            # Get predicted token indices\n","            predicted_indices = torch.argmax(outputs, dim=2)  # [B, seq_len]\n","\n","            # Decode predictions and ground truth\n","            for i in range(predicted_indices.size(0)):  # Iterate over batch\n","                pred_tokens = tokenizer.sequences_to_texts([predicted_indices[i].tolist()])[0]\n","                target_tokens = tokenizer.sequences_to_texts([captions[i].tolist()])[0]\n","\n","                predictions.append(pred_tokens)\n","                targets.append(target_tokens)\n","\n","            # Calculate accuracy\n","            # Ignore padding tokens (<PAD>) in accuracy calculation\n","            for i in range(captions.size(0)):  # Iterate over batch\n","                valid_tokens = lengths[i]  # Number of non-padding tokens\n","                total_correct += (predicted_indices[i, :valid_tokens] == captions[i, :valid_tokens]).sum().item()\n","                total_tokens += valid_tokens\n","\n","    accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    return predictions, targets, accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZb3QmzV_a6b","executionInfo":{"status":"aborted","timestamp":1738606963856,"user_tz":300,"elapsed":3,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["# Predict on validation dataset\n","test_predictions, test_ground_truth,test_accuracy = predict(model, data_loader_test, tokenizer, device)\n","\n","# Display some predictions and ground truth\n","for i in range(5):\n","    print(f\"Prediction {i + 1}: {test_predictions[i]}\")\n","    print(f\"Ground Truth {i + 1}: {test_ground_truth[i]}\")\n","    print(\"-\" * 50)\n","\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"xS7GeHCk_a6b"},"source":["## Evaluate Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5FnxQC2_a6b","executionInfo":{"status":"aborted","timestamp":1738606963856,"user_tz":300,"elapsed":3,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["\n","def compute_bleu(predictions, targets):\n","    \"\"\"\n","    Calculates the average BLEU score for all predictions.\n","\n","    Args:\n","        predictions: List of predicted sentences.\n","        targets: List of ground truth sentences.\n","\n","    Returns:\n","        avg_bleu: Average BLEU score across all predictions.\n","    \"\"\"\n","    scores = []\n","    for pred, target in zip(predictions, targets):\n","        target_tokens = target.split()\n","        pred_tokens = pred.split()\n","        score = sentence_bleu([target_tokens], pred_tokens)\n","        scores.append(score)\n","    avg_bleu = sum(scores) / len(scores)\n","    return avg_bleu\n","\n","def compute_rouge(predictions, references):\n","    \"\"\"\n","    Computes ROUGE scores for the given predictions and references.\n","\n","    Args:\n","        predictions (list): List of predicted reports (strings).\n","        references (list): List of ground truth reports (strings).\n","\n","    Returns:\n","        rouge_scores (dict): Dictionary with average ROUGE-1, ROUGE-2, and ROUGE-L scores.\n","    \"\"\"\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n","\n","    for pred, ref in tqdm(zip(predictions, references), total=len(predictions), desc=\"Computing ROUGE\"):\n","        score = scorer.score(ref, pred)\n","        for key in scores.keys():\n","            scores[key].append(score[key].fmeasure)  # Use F1-score as the metric\n","\n","    # Average the scores across all examples\n","    avg_scores = {key: sum(value) / len(value) for key, value in scores.items()}\n","    return avg_scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjw_f1fQ_a6b","executionInfo":{"status":"aborted","timestamp":1738606963856,"user_tz":300,"elapsed":3,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["# Compute ROUGE scores\n","rouge_scores = compute_rouge(test_predictions, test_ground_truth)\n","print(\"ROUGE Scores:\")\n","print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n","print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n","print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n","\n","# Compute BLEU score\n","bleu_score = compute_bleu(test_predictions, test_ground_truth)\n","print(f\"BLEU Score: {bleu_score:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUCPDS6B_a6h","executionInfo":{"status":"aborted","timestamp":1738606963856,"user_tz":300,"elapsed":3,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["def compute_f1_precision_recall(predictions, references, tokenizer):\n","    \"\"\"\n","    Computes F1-Score, Precision, and Recall for the predictions against the references.\n","\n","    Args:\n","        predictions (list): List of predicted sentences (strings).\n","        references (list): List of ground truth sentences (strings).\n","        tokenizer: Tokenizer used in the dataset for word-index mapping.\n","\n","    Returns:\n","        metrics (dict): Dictionary containing F1-Score, Precision, and Recall.\n","    \"\"\"\n","    # Flatten token-level predictions and references\n","    pred_tokens = []\n","    ref_tokens = []\n","    pad_idx = tokenizer.word_index[\"<PAD>\"]\n","\n","    for pred, ref in zip(predictions, references):\n","        pred_indices = [tokenizer.word_index.get(word, pad_idx) for word in pred.split()]\n","        ref_indices = [tokenizer.word_index.get(word, pad_idx) for word in ref.split()]\n","\n","        # Ensure both sequences have the same length\n","        min_length = min(len(pred_indices), len(ref_indices))\n","        pred_tokens.extend(pred_indices[:min_length])\n","        ref_tokens.extend(ref_indices[:min_length])\n","\n","    # Compute Precision, Recall, and F1-Score\n","    precision = precision_score(ref_tokens, pred_tokens, average=\"weighted\",zero_division=0)\n","    recall = recall_score(ref_tokens, pred_tokens, average=\"weighted\",zero_division=0)\n","    f1 = f1_score(ref_tokens, pred_tokens, average=\"weighted\",zero_division=0)\n","\n","    return {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZL6gPvr_a6h","executionInfo":{"status":"aborted","timestamp":1738606963856,"user_tz":300,"elapsed":3,"user":{"displayName":"nikhil deekonda","userId":"12985331890070385481"}}},"outputs":[],"source":["# Compute F1, Precision, and Recall\n","metrics = compute_f1_precision_recall(test_predictions, test_ground_truth, tokenizer)\n","print(f\"Precision: {metrics['Precision']:.4f}\")\n","print(f\"Recall: {metrics['Recall']:.4f}\")\n","print(f\"F1-Score: {metrics['F1-Score']:.4f}\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}